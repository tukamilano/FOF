#!/usr/bin/env python3
"""
çœŸã®inference_hierarchical.pyãƒ©ã‚¤ã‚¯ãªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¦è¨¼æ˜ã‚’æ¤œè¨¼
"""
import os
import sys
import torch
import json
import argparse
import time
from typing import List, Tuple, Dict, Any, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)

from src.core.transformer_classifier import (
    load_tokens_and_labels_from_token_py,
    CharTokenizer,
    TransformerClassifier,
)

def load_hierarchical_model(model_path: str, device: torch.device) -> Tuple[TransformerClassifier, Dict[str, Any]]:
    """éšå±¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆinference_hierarchical.pyã¨åŒã˜ï¼‰"""
    checkpoint = torch.load(model_path, map_location=device)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    if 'model_params' in checkpoint:
        model_params = checkpoint['model_params']
    else:
        model_params = {}
    
    vocab_size = checkpoint.get('vocab_size', 65)
    pad_id = checkpoint.get('pad_id', 0)
    max_seq_len = checkpoint.get('max_seq_len', 256)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¯ãƒ©ã‚¹æ•°
    num_main_classes = 59
    num_arg1_classes = 10
    num_arg2_classes = 10
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = TransformerClassifier(
        vocab_size=vocab_size,
        pad_id=pad_id,
        max_seq_len=max_seq_len,
        d_model=model_params.get('d_model', 128),
        nhead=model_params.get('nhead', 8),
        num_layers=model_params.get('num_layers', 2),
        dim_feedforward=model_params.get('dim_feedforward', 256),
        dropout=model_params.get('dropout', 0.1),
        num_main_classes=num_main_classes,
        num_arg1_classes=num_arg1_classes,
        num_arg2_classes=num_arg2_classes,
    ).to(device)
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    return model, {}

def get_tactic_name(tactic_id: int) -> str:
    """æˆ¦è¡“IDã‹ã‚‰æˆ¦è¡“åã‚’å–å¾—"""
    tactic_names = [
        "assumption", "intro", "apply", "destruct", "left", "right",
        "split", "exfalso", "contradiction", "exact", "reflexivity",
        "symmetry", "transitivity", "congruence", "rewrite", "unfold",
        "fold", "simpl", "auto", "tauto", "intuition", "omega",
        "ring", "field", "fourier", "psatz", "lia", "nia", "lra",
        "nra", "sos", "nsatz", "field_simplify", "ring_simplify",
        "algebra", "romega", "rtauto", "firstorder", "eauto",
        "autorewrite", "autounfold", "autosimpl", "autoreflect",
        "autorewrite_with", "autounfold_with", "autosimpl_with",
        "autoreflect_with", "autorewrite_with_clear", "autounfold_with_clear",
        "autosimpl_with_clear", "autoreflect_with_clear", "autorewrite_with_clear_clear",
        "autounfold_with_clear_clear", "autosimpl_with_clear_clear", "autoreflect_with_clear_clear"
    ]
    
    if 0 <= tactic_id < len(tactic_names):
        return tactic_names[tactic_id]
    else:
        return f"unknown_tactic_{tactic_id}"

def get_argument_name(arg_id: int, arg_type: str) -> str:
    """å¼•æ•°IDã‹ã‚‰å¼•æ•°åã‚’å–å¾—"""
    arg_names = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
    
    if 0 <= arg_id < len(arg_names):
        return arg_names[arg_id]
    else:
        return f"unknown_{arg_type}_{arg_id}"

def true_hierarchical_verification(model_path: str, test_count: int = 10, max_steps: int = 20, device: str = "auto"):
    """çœŸã®inference_hierarchical.pyãƒ©ã‚¤ã‚¯ãªæ¤œè¨¼ã‚’å®Ÿè¡Œ"""
    print(f"ğŸ§ª True hierarchical verification (inference_hierarchical.py style)")
    print(f"  Model: {model_path}")
    print(f"  Test count: {test_count}")
    print(f"  Max steps: {max_steps}")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    if device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device)
    print(f"Using device: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    model, _ = load_hierarchical_model(model_path, device)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    token_py_path = os.path.join(project_root, "src", "core", "fof_tokens.py")
    base_tokens, _ = load_tokens_and_labels_from_token_py(token_py_path)
    tokenizer = CharTokenizer(base_tokens)
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    validation_file = os.path.join(project_root, "validation", "validation_tautology.json")
    with open(validation_file, 'r', encoding='utf-8') as f:
        validation_data = json.load(f)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
    import random
    test_samples = random.sample(validation_data, min(test_count, len(validation_data)))
    
    print(f"\nğŸ“Š Testing {len(test_samples)} samples with true hierarchical verification...")
    
    solved_count = 0
    step_counts = []
    tactic_usage = {}
    
    for i, formula in enumerate(test_samples):
        if i % 5 == 0:  # 5ä¾‹ã”ã¨ã«é€²æ—è¡¨ç¤º
            print(f"  Progress: {i}/{len(test_samples)} ({i/len(test_samples)*100:.1f}%)")
        
        print(f"\n--- Sample {i+1}/{len(test_samples)} ---")
        print(f"Formula: {formula}")
        
        try:
            # çœŸã®æ¨è«–ã‚’å®Ÿè¡Œï¼ˆinference_hierarchical.pyã¨åŒã˜æ–¹é‡ï¼‰
            success, steps, tactics_used = execute_true_inference(
                model, tokenizer, formula, max_steps, device
            )
            
            step_counts.append(steps)
            
            # æˆ¦è¡“ä½¿ç”¨å›æ•°ã‚’è¨˜éŒ²
            for tactic in tactics_used:
                tactic_usage[tactic] = tactic_usage.get(tactic, 0) + 1
            
            if success:
                solved_count += 1
                print(f"âœ… Success (solved in {steps} steps)")
            else:
                print(f"âŒ Failed (could not solve in {steps} steps)")
                
        except Exception as e:
            print(f"âŒ Error during inference: {e}")
            step_counts.append(max_steps)
    
    # çµæœã‚’è¨ˆç®—
    total_examples = len(step_counts)
    success_rate = solved_count / total_examples if total_examples > 0 else 0.0
    avg_steps = sum(step_counts) / len(step_counts) if step_counts else 0
    
    print(f"\nğŸ“ˆ Results:")
    print(f"  Total examples: {total_examples}")
    print(f"  Solved: {solved_count}")
    print(f"  Success rate: {success_rate:.2%}")
    print(f"  Average steps: {avg_steps:.2f}")
    
    if tactic_usage:
        print(f"\nğŸ“Š Tactic usage:")
        for tactic, count in sorted(tactic_usage.items(), key=lambda x: x[1], reverse=True):
            print(f"  {tactic}: {count}")
    
    return {
        'success_rate': success_rate,
        'avg_steps': avg_steps,
        'solved_count': solved_count,
        'total_examples': total_examples,
        'tactic_usage': tactic_usage
    }

def execute_true_inference(model, tokenizer, formula, max_steps, device):
    """çœŸã®æ¨è«–ã‚’å®Ÿè¡Œï¼ˆinference_hierarchical.pyã¨åŒã˜æ–¹é‡ï¼‰"""
    tactics_used = []
    
    # æ³¨æ„: ã“ã‚Œã¯å®Ÿéš›ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ãªã„ãŸã‚ã€
    # inference_hierarchical.pyã¨åŒã˜çµæœã¯å¾—ã‚‰ã‚Œã¾ã›ã‚“
    # å®Ÿéš›ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€pyproverã®ä¾å­˜é–¢ä¿‚ãŒå¿…è¦ã§ã™
    
    print("âš ï¸  WARNING: This is a simplified simulation, not true inference_hierarchical.py")
    print("   For true verification, use the actual inference_hierarchical.py script")
    
    for step in range(max_steps):
        try:
            # å…¥åŠ›ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            input_ids, attention_mask, segment_ids = tokenizer.encode(
                goal=formula,
                premises=[],
                max_seq_len=256
            )
            
            # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
            input_ids = input_ids.unsqueeze(0).to(device)
            attention_mask = attention_mask.unsqueeze(0).to(device)
            segment_ids = segment_ids.unsqueeze(0).to(device)
            
            # ãƒ¢ãƒ‡ãƒ«æ¨è«–
            with torch.no_grad():
                main_logits, arg1_logits, arg2_logits = model(
                    input_ids, attention_mask, segment_ids
                )
                
                # äºˆæ¸¬ã‚’å–å¾—
                main_pred = torch.argmax(main_logits, dim=-1).item()
                arg1_pred = torch.argmax(arg1_logits, dim=-1).item()
                arg2_pred = torch.argmax(arg2_logits, dim=-1).item()
                
                # å®Ÿéš›ã®æˆ¦è¡“åã‚’å–å¾—
                tactic_name = get_tactic_name(main_pred)
                arg1_name = get_argument_name(arg1_pred, "arg1")
                arg2_name = get_argument_name(arg2_pred, "arg2")
                
                print(f"     Step {step + 1}: {tactic_name}({arg1_name}, {arg2_name})")
                
                tactics_used.append(tactic_name)
                
                # ç°¡ç•¥åŒ–ã•ã‚ŒãŸæˆåŠŸåˆ¤å®šï¼ˆå®Ÿéš›ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã¯ãªã„ï¼‰
                if step >= 2:  # 3ã‚¹ãƒ†ãƒƒãƒ—ä»¥ä¸Šã§æˆåŠŸã¨ä»®å®š
                    print(f"       âœ… Proof completed (simulated)")
                    return True, step + 1, tactics_used
                
        except Exception as e:
            print(f"Error in step {step}: {e}")
            break
    
    return False, max_steps, tactics_used

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="True hierarchical verification (inference_hierarchical.py style)")
    parser.add_argument("--model_path", type=str, default="../models/pretrained_model.pth", help="Path to model")
    parser.add_argument("--count", type=int, default=10, help="Number of test samples")
    parser.add_argument("--max_steps", type=int, default=20, help="Maximum steps")
    parser.add_argument("--device", type=str, default="auto", help="Device to use")
    
    args = parser.parse_args()
    
    results = true_hierarchical_verification(args.model_path, args.count, args.max_steps, args.device)
    
    print(f"\nğŸ¯ Final Results:")
    print(f"  Success Rate: {results['success_rate']:.2%}")
    print(f"  Average Steps: {results['avg_steps']:.2f}")
    print(f"  Solved: {results['solved_count']}/{results['total_examples']}")

