#!/usr/bin/env python3
"""
Actor-Criticãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã•ã‚ŒãŸä»®ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
"""
import os
import sys
import torch
import argparse

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)

from src.core.transformer_classifier import TransformerClassifier
from src.core.actor_critic_model import ActorCriticModel

def create_dummy_actor_critic_model(output_path: str, vocab_size: int = 65, d_model: int = 128):
    """åˆæœŸåŒ–ã•ã‚ŒãŸActor-Criticãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    print(f"ğŸ—ï¸  Creating dummy Actor-Critic model...")
    print(f"  Output path: {output_path}")
    print(f"  Vocab size: {vocab_size}")
    print(f"  D model: {d_model}")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¯ãƒ©ã‚¹æ•°
    num_main_classes = 59
    num_arg1_classes = 10
    num_arg2_classes = 10
    
    # ãƒ™ãƒ¼ã‚¹Transformerã‚’ä½œæˆ
    base_transformer = TransformerClassifier(
        vocab_size=vocab_size,
        pad_id=0,
        max_seq_len=256,
        d_model=d_model,
        nhead=8,
        num_layers=2,
        dim_feedforward=256,
        dropout=0.1,
        num_main_classes=num_main_classes,
        num_arg1_classes=num_arg1_classes,
        num_arg2_classes=num_arg2_classes,
    )
    
    # Actor-Criticãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    actor_critic_model = ActorCriticModel(
        base_transformer=base_transformer,
        pretrained_model=base_transformer,  # åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’pretrained_modelã¨ã—ã¦ä½¿ç”¨
        critic_hidden_dim=512
    )
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
    checkpoint = {
        'model_state_dict': actor_critic_model.state_dict(),
        'vocab_size': vocab_size,
        'pad_id': 0,
        'max_seq_len': 256,
        'model_params': {
            'd_model': d_model,
            'nhead': 8,
            'num_layers': 2,
            'dim_feedforward': 256,
            'dropout': 0.1,
        }
    }
    
    # ä¿å­˜
    torch.save(checkpoint, output_path)
    print(f"âœ… Dummy Actor-Critic model saved to: {output_path}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    total_params = sum(p.numel() for p in actor_critic_model.parameters())
    trainable_params = sum(p.numel() for p in actor_critic_model.parameters() if p.requires_grad)
    
    print(f"\nğŸ“Š Model Information:")
    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")
    print(f"  Vocab size: {vocab_size}")
    print(f"  D model: {d_model}")
    print(f"  Main classes: {num_main_classes}")
    print(f"  Arg1 classes: {num_arg1_classes}")
    print(f"  Arg2 classes: {num_arg2_classes}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create dummy Actor-Critic model")
    parser.add_argument("--output_path", type=str, default="../models/actor_critic_model.pth", help="Output path for the model")
    parser.add_argument("--vocab_size", type=int, default=65, help="Vocabulary size")
    parser.add_argument("--d_model", type=int, default=128, help="Model dimension")
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
    
    create_dummy_actor_critic_model(args.output_path, args.vocab_size, args.d_model)
