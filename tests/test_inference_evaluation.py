#!/usr/bin/env python3
"""
æ¨è«–è©•ä¾¡ã®ã¿ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ç•°ãªã‚‹difficultyå€¤ã§ã®æ¨è«–æ€§èƒ½ã‚’æ¯”è¼ƒ
"""
from __future__ import annotations

import argparse
import os
import sys
import time
from typing import Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)

import torch
import torch.nn as nn

from src.core.transformer_classifier import (
    load_tokens_and_labels_from_token_py,
    CharTokenizer,
    TransformerClassifier,
    build_hierarchical_label_mappings,
)
from src.core.generate_prop import FormulaGenerator, filter_formulas
from src.core.parameter import (
    get_model_params, get_training_params, 
    get_system_params, get_hierarchical_labels, DeviceType
)
from src.training.inference_hierarchical import evaluate_inference_performance


def test_difficulty_impact():
    """ç•°ãªã‚‹difficultyå€¤ã§ã®æ¨è«–æ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    model_params = get_model_params()
    training_params = get_training_params()
    system_params = get_system_params()
    hierarchical_labels = get_hierarchical_labels()
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿
    token_py_path = os.path.join(project_root, "src", "core", "fof_tokens.py")
    print(f"Loading tokens from: {token_py_path}")
    base_tokens, _ = load_tokens_and_labels_from_token_py(token_py_path)
    
    # éšå±¤åˆ†é¡ç”¨ã®ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
    main_to_id, arg1_to_id, arg2_to_id, id_to_main, id_to_arg1, id_to_arg2 = build_hierarchical_label_mappings(
        hierarchical_labels.main_tactics,
        hierarchical_labels.arg1_values,
        hierarchical_labels.arg2_values
    )
    
    print(f"Main tactics: {len(id_to_main)} classes")
    print(f"Arg1 values: {len(id_to_arg1)} classes")
    print(f"Arg2 values: {len(id_to_arg2)} classes")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    tokenizer = CharTokenizer(
        base_tokens=base_tokens,
        add_tactic_tokens=model_params.add_tactic_tokens,
        num_tactic_tokens=model_params.num_tactic_tokens
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼‰
    model = TransformerClassifier(
        vocab_size=tokenizer.vocab_size,
        pad_id=tokenizer.pad_id,
        max_seq_len=512,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    model = model.to(device)
    model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
    
    # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    label_mappings = {
        'main_to_id': main_to_id,
        'arg1_to_id': arg1_to_id,
        'arg2_to_id': arg2_to_id,
        'id_to_main': id_to_main,
        'id_to_arg1': id_to_arg1,
        'id_to_arg2': id_to_arg2
    }
    
    # ç•°ãªã‚‹difficultyå€¤ã§ãƒ†ã‚¹ãƒˆ
    difficulty_values = [0.3, 0.5, 0.7, 0.9]
    num_examples = 50
    max_steps = 20
    
    print(f"\nğŸ§ª Testing inference performance with different difficulty values")
    print(f"ğŸ“Š Number of examples per test: {num_examples}")
    print(f"ğŸ“Š Max steps per example: {max_steps}")
    print("=" * 80)
    
    results = {}
    
    for difficulty in difficulty_values:
        print(f"\nğŸ” Testing difficulty = {difficulty}")
        print("-" * 40)
        
        # æŒ‡å®šã•ã‚ŒãŸdifficultyã§ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ã‚’ç”Ÿæˆ
        gen = FormulaGenerator(
            variables=["a", "b", "c"],
            allow_const=False,
            difficulty=difficulty,
            max_depth=4,
            seed=42  # å†ç¾æ€§ã®ãŸã‚å›ºå®šã‚·ãƒ¼ãƒ‰
        )
        
        # ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ã‚’ç”Ÿæˆã—ã¦è¡¨ç¤º
        tautologies = filter_formulas(
            gen=gen,
            max_len=100,
            require_tautology=True,
            limit=num_examples
        )
        
        if not tautologies:
            print(f"âŒ Failed to generate tautologies for difficulty {difficulty}")
            continue
        
        print(f"Generated {len(tautologies)} tautologies for difficulty {difficulty}")
        print("First 5 tautologies:")
        for i in range(min(5, len(tautologies))):
            print(f"  {i+1}: {tautologies[i]}")
        
        # æ¨è«–æ€§èƒ½ã‚’è©•ä¾¡
        start_time = time.time()
        success_rate, avg_steps = evaluate_inference_performance(
            model, tokenizer, label_mappings, device, 512,
            num_examples=num_examples, 
            max_steps=max_steps, 
            temperature=1.0,
            difficulty=difficulty,  # åŒã˜difficultyå€¤ã‚’ä½¿ç”¨
            seed=42,  # åŒã˜ã‚·ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
            max_depth=4  # åŒã˜max_depthå€¤ã‚’ä½¿ç”¨
        )
        eval_time = time.time() - start_time
        
        print(f"  Success rate: {success_rate:.3f}")
        print(f"  Avg steps (when solved): {avg_steps:.2f}")
        print(f"  Evaluation time: {eval_time:.2f}s")
        
        results[difficulty] = {
            'success_rate': success_rate,
            'avg_steps': avg_steps,
            'eval_time': eval_time,
            'tautologies': tautologies[:5]  # æœ€åˆã®5å€‹ã‚’ä¿å­˜
        }
    
    # çµæœã‚’ã¾ã¨ã‚ã¦è¡¨ç¤º
    print(f"\nğŸ“Š Summary of Results")
    print("=" * 80)
    print(f"{'Difficulty':<12} {'Success Rate':<15} {'Avg Steps':<12} {'Eval Time':<12}")
    print("-" * 80)
    
    for difficulty in difficulty_values:
        if difficulty in results:
            r = results[difficulty]
            print(f"{difficulty:<12.1f} {r['success_rate']:<15.3f} {r['avg_steps']:<12.2f} {r['eval_time']:<12.2f}")
    
    # ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ã®è¤‡é›‘ã•ã‚’åˆ†æ
    print(f"\nğŸ” Tautology Complexity Analysis")
    print("=" * 80)
    
    for difficulty in difficulty_values:
        if difficulty in results:
            print(f"\nDifficulty {difficulty} - First 5 tautologies:")
            for i, tautology in enumerate(results[difficulty]['tautologies']):
                print(f"  {i+1}: {tautology} (length: {len(tautology)})")
    
    return results


def test_tautology_generation_detailed():
    """ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ç”Ÿæˆã®è©³ç´°ãƒ†ã‚¹ãƒˆ"""
    
    print(f"\nğŸ” Detailed Tautology Generation Test")
    print("=" * 80)
    
    difficulty_values = [0.3, 0.5, 0.7, 0.9]
    num_examples = 20
    
    for difficulty in difficulty_values:
        print(f"\nğŸ“Š Difficulty = {difficulty}")
        print("-" * 40)
        
        gen = FormulaGenerator(
            variables=["a", "b", "c"],
            allow_const=False,
            difficulty=difficulty,
            max_depth=4,
            seed=42
        )
        
        tautologies = filter_formulas(
            gen=gen,
            max_len=100,
            require_tautology=True,
            limit=num_examples
        )
        
        if tautologies:
            # çµ±è¨ˆã‚’è¨ˆç®—
            lengths = [len(t) for t in tautologies]
            avg_length = sum(lengths) / len(lengths)
            max_length = max(lengths)
            min_length = min(lengths)
            
            print(f"Generated {len(tautologies)} tautologies")
            print(f"Average length: {avg_length:.1f}")
            print(f"Min length: {min_length}")
            print(f"Max length: {max_length}")
            print("All generated tautologies:")
            for i, tautology in enumerate(tautologies):
                print(f"  {i+1:2d}: {tautology}")
        else:
            print("âŒ Failed to generate tautologies")


def main():
    parser = argparse.ArgumentParser(description="Test inference evaluation with different difficulty values")
    parser.add_argument("--num_examples", type=int, default=50, help="number of examples for evaluation")
    parser.add_argument("--max_steps", type=int, default=30, help="max steps for inference")
    parser.add_argument("--test_generation", action="store_true", help="test tautology generation in detail")
    args = parser.parse_args()
    
    print("ğŸš€ Inference Evaluation Test")
    print(f"   Script: {sys.argv[0]}")
    print(f"   Arguments: {' '.join(sys.argv[1:])}")
    print("=" * 80)
    
    # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š
    import random
    import numpy as np
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)
    
    # æ¨è«–æ€§èƒ½ãƒ†ã‚¹ãƒˆ
    results = test_difficulty_impact()
    
    # è©³ç´°ãªç”Ÿæˆãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if args.test_generation:
        test_tautology_generation_detailed()
    
    print(f"\nğŸ‰ Test completed!")
    print("=" * 80)


if __name__ == "__main__":
    main()
