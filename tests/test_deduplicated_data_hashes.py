#!/usr/bin/env python3
"""
Test to check duplicates of state_hash and state_tactic_hash in deduplicated_data
"""

import json
import os
import glob
from collections import Counter, defaultdict
from typing import Dict, List, Set, Tuple

def check_deduplicated_data_hashes():
    """Check state_hash and state_tactic_hash duplicates in all files in deduplicated_data"""
    # Get JSON files in deduplicated_data directory
    pattern = "deduplicated_data/*.json"
    files = glob.glob(pattern)
    
    if not files:
        print("No JSON files found in deduplicated_data/")
        return
    
    print(f"Found {len(files)} files in deduplicated_data/")
    
    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥ é›†ã‚ã‚‹
    global_state_hash_counter = Counter()
    global_state_tactic_hash_counter = Counter()
    global_state_hash_files = defaultdict(set)  # ãƒãƒƒã‚·ãƒ¥ ã©ã®ãƒ•ã‚¡ã‚¤ãƒ« å«ã¾ã‚Œã¦existsã‹ è¿½è·¡
    global_state_tactic_hash_files = defaultdict(set)
    
    total_steps = 0
    total_files = 0
    
    for file_path in sorted(files):
        try:
            print(f"Reading {file_path}...")
            with open(file_path, 'r', encoding='utf-8') as f:
                steps = json.load(f)
            
            print(f"Processing {len(steps)} steps...")
            total_steps += len(steps)
            total_files += 1
            
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒãƒƒã‚·ãƒ¥ ãƒã‚§ãƒƒã‚¯
            for step in steps:
                state_hash = step.get('state_hash', '')
                state_tactic_hash = step.get('state_tactic_hash', '')
                
                if state_hash:
                    global_state_hash_counter[state_hash] += 1
                    global_state_hash_files[state_hash].add(os.path.basename(file_path))
                
                if state_tactic_hash:
                    global_state_tactic_hash_counter[state_tactic_hash] += 1
                    global_state_tactic_hash_files[state_tactic_hash].add(os.path.basename(file_path))
            
            print(f"  {os.path.basename(file_path)}: {len(steps)} steps processed")
            
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            continue
    
    # é‡è¤‡ æ¤œå‡º
    duplicate_state_hashes = {h: count for h, count in global_state_hash_counter.items() if count > 1}
    duplicate_state_tactic_hashes = {h: count for h, count in global_state_tactic_hash_counter.items() if count > 1}
    
    # Detect cross-file duplicatesï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ« ã¾ãŸ ã‚‹é‡è¤‡ï¼‰
    cross_file_state_hash_duplicates = {h: files for h, files in global_state_hash_files.items() if len(files) > 1}
    cross_file_state_tactic_hash_duplicates = {h: files for h, files in global_state_tactic_hash_files.items() if len(files) > 1}
    
    # çµæœ è¡¨ç¤º
    print(f"\n" + "="*80)
    print("deduplicated_data ãƒãƒƒã‚·ãƒ¥é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ")
    print("="*80)
    
    print(f"å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
    print(f"ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps:,}")
    
    print(f"\nstate_hashçµ±è¨ˆ:")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ state_hash: {len(global_state_hash_counter):,}")
    print(f"  é‡è¤‡ state_hash: {len(duplicate_state_hashes):,}")
    print(f"  ãƒ•ã‚¡ã‚¤ãƒ«é–“é‡è¤‡: {len(cross_file_state_hash_duplicates):,}")
    
    print(f"\nstate_tactic_hashçµ±è¨ˆ:")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ state_tactic_hash: {len(global_state_tactic_hash_counter):,}")
    print(f"  é‡è¤‡ state_tactic_hash: {len(duplicate_state_tactic_hashes):,}")
    print(f"  ãƒ•ã‚¡ã‚¤ãƒ«é–“é‡è¤‡: {len(cross_file_state_tactic_hash_duplicates):,}")
    
    # state_hashé‡è¤‡ã®è©³ç´°è¡¨ç¤º
    if duplicate_state_hashes:
        print(f"\né‡è¤‡state_hashã®è©³ç´° (ä¸Šä½10ä»¶):")
        for i, (hash_value, count) in enumerate(list(duplicate_state_hashes.items())[:10]):
            files_list = list(global_state_hash_files[hash_value])
            cross_file_indicator = " (ãƒ•ã‚¡ã‚¤ãƒ«é–“)" if len(files_list) > 1 else " (ãƒ•ã‚¡ã‚¤ãƒ«å†…)"
            print(f"  {i+1}. Hash: {hash_value[:16]}... Count: {count}{cross_file_indicator}, Files: {files_list}")
    else:
        print(f"\nâœ“ state_hashé‡è¤‡ã¯è¦‹ã‹ã‚Šã¾ã›ã‚“ with/at didï¼")
    
    # state_tactic_hashé‡è¤‡ã®è©³ç´°è¡¨ç¤º
    if duplicate_state_tactic_hashes:
        print(f"\né‡è¤‡state_tactic_hashã®è©³ç´° (ä¸Šä½10ä»¶):")
        for i, (hash_value, count) in enumerate(list(duplicate_state_tactic_hashes.items())[:10]):
            files_list = list(global_state_tactic_hash_files[hash_value])
            cross_file_indicator = " (ãƒ•ã‚¡ã‚¤ãƒ«é–“)" if len(files_list) > 1 else " (ãƒ•ã‚¡ã‚¤ãƒ«å†…)"
            print(f"  {i+1}. Hash: {hash_value[:16]}... Count: {count}{cross_file_indicator}, Files: {files_list}")
    else:
        print(f"\nâœ“ state_tactic_hashé‡è¤‡ã¯è¦‹ã‹ã‚Šã¾ã›ã‚“ with/at didï¼")
    
    # æœŸå¾…is doneçµæœã®ç¢ºèª
    print(f"\n" + "="*80)
    print("æœŸå¾…is doneçµæœã®ç¢ºèª")
    print("="*80)
    
    # state_hashã®é‡è¤‡ã¯æ­£å¸¸ï¼ˆåŒã˜çŠ¶æ…‹ with/at ç•°becometactic è©¦ã™ãŸã‚ï¼‰
    if len(duplicate_state_hashes) > 0:
        print(f"â„¹ï¸  state_hash: {len(duplicate_state_hashes)}ã®é‡è¤‡ï¼ˆæ­£å¸¸ - åŒã˜çŠ¶æ…‹ with/at ç•°becometactic è©¦ã™ãŸã‚ï¼‰")
        print(f"   - ãƒ•ã‚¡ã‚¤ãƒ«é–“é‡è¤‡: {len(cross_file_state_hash_duplicates)}")
    else:
        print("âœ… state_hash: é‡è¤‡ãªã—")
    
    # state_tactic_hashã®é‡è¤‡ã¯é™¤å»is doneã¹ã
    if len(duplicate_state_tactic_hashes) == 0:
        print("âœ… state_tactic_hash: é‡è¤‡ãªã—ï¼ˆæœŸå¾…é€šã‚Š - é‡è¤‡é™¤å» æ­£ã—ãå‹•ä½œï¼‰")
    else:
        print(f"âŒ state_tactic_hash: {len(duplicate_state_tactic_hashes)}ã®é‡è¤‡ æ¤œå‡ºã•ã‚Œã¾didï¼ˆé‡è¤‡é™¤å» å•é¡Œã‚ã‚Šï¼‰")
        print(f"   - ãƒ•ã‚¡ã‚¤ãƒ«é–“é‡è¤‡: {len(cross_file_state_tactic_hash_duplicates)}")
    
    return {
        'total_files': total_files,
        'total_steps': total_steps,
        'unique_state_hashes': len(global_state_hash_counter),
        'unique_state_tactic_hashes': len(global_state_tactic_hash_counter),
        'duplicate_state_hashes': len(duplicate_state_hashes),
        'duplicate_state_tactic_hashes': len(duplicate_state_tactic_hashes),
        'cross_file_state_hash_duplicates': len(cross_file_state_hash_duplicates),
        'cross_file_state_tactic_hash_duplicates': len(cross_file_state_tactic_hash_duplicates)
    }

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ” deduplicated_data ãƒãƒƒã‚·ãƒ¥é‡è¤‡ãƒã‚§ãƒƒã‚¯ é–‹å§‹...")
    result = check_deduplicated_data_hashes()
    
    if result:
        print(f"\nğŸ“Š ãƒã‚§ãƒƒã‚¯å®Œäº†:")
        print(f"  å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {result['total_files']}")
        print(f"  ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {result['total_steps']:,}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ state_hash: {result['unique_state_hashes']:,}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ state_tactic_hash: {result['unique_state_tactic_hashes']:,}")
        print(f"  é‡è¤‡ state_hash: {result['duplicate_state_hashes']}")
        print(f"  é‡è¤‡ state_tactic_hash: {result['duplicate_state_tactic_hashes']}")

if __name__ == "__main__":
    main()
