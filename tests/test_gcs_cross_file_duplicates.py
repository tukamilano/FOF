#!/usr/bin/env python3
"""
GCSã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨ªæ–­ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import hashlib
import os
import sys
from collections import Counter, defaultdict
from typing import Dict, List, Set, Tuple
from google.cloud import storage
import argparse

def example_hash(original_goal: str) -> str:
    """Exampleå…¨ä½“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆå…ƒã®ç›®æ¨™å¼ã®ã¿ï¼‰"""
    return hashlib.md5(original_goal.encode()).hexdigest()

def list_gcs_files(bucket_name: str, prefix: str) -> List[str]:
    """GCSãƒã‚±ãƒƒãƒˆã‹ã‚‰æŒ‡å®šãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    
    blobs = bucket.list_blobs(prefix=prefix)
    files = []
    
    for blob in blobs:
        if blob.name.endswith('.json'):
            files.append(blob.name)
    
    return sorted(files)

def download_gcs_file(bucket_name: str, file_path: str) -> List[Dict]:
    """GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦JSONã¨ã—ã¦èª­ã¿è¾¼ã¿"""
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(file_path)
    
    content = blob.download_as_text()
    return json.loads(content)

def check_gcs_cross_file_duplicates(bucket_name: str, prefix: str, limit: int = None):
    """GCSã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨ªæ–­ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
    print(f"GCS Bucket: {bucket_name}")
    print(f"Prefix: {prefix}")
    if limit:
        print(f"Limit: æœ€åˆã® {limit} ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†")
    print("="*60)
    
    # GCSãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    print("GCSãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ä¸­...")
    files = list_gcs_files(bucket_name, prefix)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã‚’é©ç”¨
    if limit and limit > 0:
        files = files[:limit]
        print(f"åˆ¶é™é©ç”¨: {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™")
    
    if not files:
        print("No JSON files found in GCS bucket")
        return
    
    print(f"Found {len(files)} files in GCS bucket")
    
    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥ã‚’é›†ã‚ã‚‹
    global_example_hash_counter = Counter()
    global_example_hash_files = defaultdict(set)  # ãƒãƒƒã‚·ãƒ¥ãŒã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’è¿½è·¡
    file_stats = []  # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®çµ±è¨ˆã‚’ä¿å­˜
    
    total_examples = 0
    total_steps = 0
    processed_files = 0
    
    for file_path in files:
        try:
            print(f"Processing {os.path.basename(file_path)}...")
            examples = download_gcs_file(bucket_name, file_path)
            
            file_examples = len(examples)
            file_steps = sum(len(example.get('steps', [])) for example in examples)
            
            total_examples += file_examples
            total_steps += file_steps
            processed_files += 1
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ã™ã¹ã¦ã®ãƒãƒƒã‚·ãƒ¥ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã«è¿½åŠ 
            for example in examples:
                example_hash_val = example.get('example_hash', '')
                if not example_hash_val:
                    original_goal = example.get('meta', {}).get('goal_original', '')
                    if original_goal:
                        example_hash_val = example_hash(original_goal)
                
                if example_hash_val:
                    global_example_hash_counter[example_hash_val] += 1
                    global_example_hash_files[example_hash_val].add(os.path.basename(file_path))
            
            print(f"  {os.path.basename(file_path)}: {file_examples} examples, {file_steps} steps")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã‚’ä¿å­˜
            file_stats.append({
                'file_path': file_path,
                'examples': file_examples,
                'steps': file_steps
            })
            
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            continue
    
    # é‡è¤‡ã‚’æ¤œå‡º
    duplicates = {h: count for h, count in global_example_hash_counter.items() if count > 1}
    duplicate_count = len(duplicates)
    
    # å…¨ä½“ã®çµæœã‚’è¡¨ç¤º
    print(f"\n" + "="*60)
    print("GCSå…¨ä½“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ")
    print("="*60)
    
    print(f"å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {processed_files}")
    print(f"ç·ä¾‹æ•°: {total_examples:,}")
    print(f"ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps:,}")
    
    print(f"\nãƒãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ example_hash: {len(global_example_hash_counter):,}")
    
    print(f"\né‡è¤‡æ¤œå‡º:")
    print(f"  é‡è¤‡ example_hash: {duplicate_count:,}")
    
    # é‡è¤‡ã®è©³ç´°è¡¨ç¤º
    if duplicate_count > 0:
        print(f"\né‡è¤‡Exampleã®è©³ç´°:")
        for hash_value, count in list(duplicates.items())[:20]:
            files_list = sorted(global_example_hash_files[hash_value])
            print(f"  {hash_value} (å‡ºç¾å›æ•°: {count}, ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(files_list)})")
        if duplicate_count > 20:
            print(f"  ... ä»– {duplicate_count - 20} å€‹")
    else:
        print(f"\nâœ“ é‡è¤‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")
    
    # é‡è¤‡ç‡ã®è¨ˆç®—
    if total_examples > 0:
        duplicate_examples = sum(count - 1 for count in duplicates.values())
        duplicate_rate = (duplicate_examples / total_examples) * 100
        print(f"\né‡è¤‡ç‡: {duplicate_rate:.2f}% ({duplicate_examples:,}/{total_examples:,})")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®é‡è¤‡çµ±è¨ˆ
        print(f"\nğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®é‡è¤‡çµ±è¨ˆ:")
        for i, stat in enumerate(file_stats, 1):
            file_duplicates = sum(1 for hash_val, (count, files_list) in duplicates.items() 
                                if count > 1 and os.path.basename(stat['file_path']) in files_list)
            file_examples = stat['examples']
            if file_examples > 0:
                file_duplicate_rate = (file_duplicates / file_examples) * 100
                print(f"  File {i:02d}: {file_duplicates}/{file_examples} duplicates ({file_duplicate_rate:.1f}%) - {os.path.basename(stat['file_path'])}")
            else:
                print(f"  File {i:02d}: 0/0 duplicates (N/A) - {os.path.basename(stat['file_path'])}")

def main():
    parser = argparse.ArgumentParser(description='GCSã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨ªæ–­ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ')
    parser.add_argument('--bucket', default='fof-data-20251009-milano', help='GCS bucket name')
    parser.add_argument('--prefix', default='generated_data/', help='GCS prefix')
    parser.add_argument('--limit', type=int, help='å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã®ä¸Šé™ï¼ˆæœ€åˆã®Nå€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰')
    
    args = parser.parse_args()
    
    check_gcs_cross_file_duplicates(args.bucket, args.prefix, args.limit)

if __name__ == "__main__":
    main()
