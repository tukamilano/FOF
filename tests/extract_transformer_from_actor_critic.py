#!/usr/bin/env python3
"""
Actor-Criticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰TransformerClassifierã‚’æŠ½å‡ºã—ã¦è©•ä¾¡ç”¨ã«ä¿å­˜
"""
import os
import sys
import torch
import argparse
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(__file__)
sys.path.insert(0, project_root)

from src.core.transformer_classifier import (
    load_tokens_and_labels_from_token_py,
    CharTokenizer,
    TransformerClassifier,
    build_hierarchical_label_mappings,
)
from src.core.parameter import get_model_params, get_hierarchical_labels
from src.core.actor_critic_model import ActorCriticModel


def extract_transformer_from_actor_critic(
    actor_critic_path: str,
    output_path: str,
    pretrained_model_path: str = "models/pretrained_model.pth"
):
    """
    Actor-Criticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰TransformerClassifierã‚’æŠ½å‡º
    
    Args:
        actor_critic_path: Actor-Criticãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ‘ã‚¹
        pretrained_model_path: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ç”¨ï¼‰
    """
    print(f"ğŸ”„ Extracting TransformerClassifier from Actor-Critic model...")
    print(f"  Input: {actor_critic_path}")
    print(f"  Output: {output_path}")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    model_params = get_model_params()
    hierarchical_labels = get_hierarchical_labels()
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿
    token_py_path = os.path.join(project_root, "src", "core", "fof_tokens.py")
    base_tokens, _ = load_tokens_and_labels_from_token_py(token_py_path)
    
    # éšå±¤åˆ†é¡ç”¨ã®ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
    main_to_id, arg1_to_id, arg2_to_id, id_to_main, id_to_arg1, id_to_arg2 = build_hierarchical_label_mappings(
        hierarchical_labels.main_tactics,
        hierarchical_labels.arg1_values,
        hierarchical_labels.arg2_values
    )
    
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ç”¨ï¼‰
    pretrained_model = TransformerClassifier(
        vocab_size=len(base_tokens),
        pad_id=0,
        max_seq_len=256,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    if os.path.exists(pretrained_model_path):
        state_dict = torch.load(pretrained_model_path, map_location=device)
        pretrained_model.load_state_dict(state_dict)
        pretrained_model = pretrained_model.to(device)
        print("âœ… Pretrained model loaded for reference")
    
    # Actor-Criticãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    actor_critic_model = ActorCriticModel(
        base_transformer=pretrained_model,
        pretrained_model=pretrained_model,
        critic_hidden_dim=512
    ).to(device)
    
    # Actor-Criticãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿
    if os.path.exists(actor_critic_path):
        actor_critic_state_dict = torch.load(actor_critic_path, map_location=device)
        actor_critic_model.load_state_dict(actor_critic_state_dict)
        print("âœ… Actor-Critic model loaded")
    else:
        print(f"âŒ Actor-Critic model not found: {actor_critic_path}")
        return
    
    # TransformerClassifierã‚’æŠ½å‡ºï¼ˆshared_encoderã®é‡ã¿ã‚’ä½¿ç”¨ï¼‰
    transformer_model = TransformerClassifier(
        vocab_size=len(base_tokens),
        pad_id=0,
        max_seq_len=256,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    # Actor-Criticã®shared_encoderã®é‡ã¿ã‚’TransformerClassifierã«ã‚³ãƒ”ãƒ¼
    transformer_model.load_state_dict(actor_critic_model.shared_encoder.state_dict())
    transformer_model = transformer_model.to(device)
    
    # è©•ä¾¡ç”¨ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
    checkpoint = {
        'model_params': {
            'vocab_size': len(base_tokens),
            'pad_id': 0,
            'max_seq_len': 256,
            'd_model': model_params.d_model,
            'nhead': model_params.nhead,
            'num_layers': model_params.num_layers,
            'dim_feedforward': model_params.dim_feedforward,
            'dropout': model_params.dropout,
        },
        'vocab_size': len(base_tokens),
        'pad_id': 0,
        'max_seq_len': 256,
        'main_to_id': main_to_id,
        'arg1_to_id': arg1_to_id,
        'arg2_to_id': arg2_to_id,
        'id_to_main': id_to_main,
        'id_to_arg1': id_to_arg1,
        'id_to_arg2': id_to_arg2,
    }
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’è¿½åŠ 
    checkpoint.update(transformer_model.state_dict())
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    torch.save(checkpoint, output_path)
    
    print(f"âœ… TransformerClassifier extracted and saved to: {output_path}")
    print(f"ğŸ“Š Model info:")
    print(f"  Vocab size: {len(base_tokens)}")
    print(f"  Main tactics: {len(id_to_main)}")
    print(f"  Arg1 values: {len(id_to_arg1)}")
    print(f"  Arg2 values: {len(id_to_arg2)}")


def main():
    parser = argparse.ArgumentParser(description="Extract TransformerClassifier from Actor-Critic model")
    parser.add_argument("--actor_critic_path", type=str, required=True, help="Actor-Critic model path")
    parser.add_argument("--output_path", type=str, required=True, help="Output path for TransformerClassifier")
    parser.add_argument("--pretrained_model", type=str, default="models/pretrained_model.pth", help="Pretrained model path")
    
    args = parser.parse_args()
    
    extract_transformer_from_actor_critic(
        actor_critic_path=args.actor_critic_path,
        output_path=args.output_path,
        pretrained_model_path=args.pretrained_model
    )


if __name__ == "__main__":
    main()
"""
Actor-Criticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰TransformerClassifierã‚’æŠ½å‡ºã—ã¦è©•ä¾¡ç”¨ã«ä¿å­˜
"""
import os
import sys
import torch
import argparse
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(__file__)
sys.path.insert(0, project_root)

from src.core.transformer_classifier import (
    load_tokens_and_labels_from_token_py,
    CharTokenizer,
    TransformerClassifier,
    build_hierarchical_label_mappings,
)
from src.core.parameter import get_model_params, get_hierarchical_labels
from src.core.actor_critic_model import ActorCriticModel


def extract_transformer_from_actor_critic(
    actor_critic_path: str,
    output_path: str,
    pretrained_model_path: str = "models/pretrained_model.pth"
):
    """
    Actor-Criticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰TransformerClassifierã‚’æŠ½å‡º
    
    Args:
        actor_critic_path: Actor-Criticãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ‘ã‚¹
        pretrained_model_path: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ç”¨ï¼‰
    """
    print(f"ğŸ”„ Extracting TransformerClassifier from Actor-Critic model...")
    print(f"  Input: {actor_critic_path}")
    print(f"  Output: {output_path}")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    model_params = get_model_params()
    hierarchical_labels = get_hierarchical_labels()
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿
    token_py_path = os.path.join(project_root, "src", "core", "fof_tokens.py")
    base_tokens, _ = load_tokens_and_labels_from_token_py(token_py_path)
    
    # éšå±¤åˆ†é¡ç”¨ã®ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
    main_to_id, arg1_to_id, arg2_to_id, id_to_main, id_to_arg1, id_to_arg2 = build_hierarchical_label_mappings(
        hierarchical_labels.main_tactics,
        hierarchical_labels.arg1_values,
        hierarchical_labels.arg2_values
    )
    
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ç”¨ï¼‰
    pretrained_model = TransformerClassifier(
        vocab_size=len(base_tokens),
        pad_id=0,
        max_seq_len=256,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    if os.path.exists(pretrained_model_path):
        state_dict = torch.load(pretrained_model_path, map_location=device)
        pretrained_model.load_state_dict(state_dict)
        pretrained_model = pretrained_model.to(device)
        print("âœ… Pretrained model loaded for reference")
    
    # Actor-Criticãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    actor_critic_model = ActorCriticModel(
        base_transformer=pretrained_model,
        pretrained_model=pretrained_model,
        critic_hidden_dim=512
    ).to(device)
    
    # Actor-Criticãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿
    if os.path.exists(actor_critic_path):
        actor_critic_state_dict = torch.load(actor_critic_path, map_location=device)
        actor_critic_model.load_state_dict(actor_critic_state_dict)
        print("âœ… Actor-Critic model loaded")
    else:
        print(f"âŒ Actor-Critic model not found: {actor_critic_path}")
        return
    
    # TransformerClassifierã‚’æŠ½å‡ºï¼ˆshared_encoderã®é‡ã¿ã‚’ä½¿ç”¨ï¼‰
    transformer_model = TransformerClassifier(
        vocab_size=len(base_tokens),
        pad_id=0,
        max_seq_len=256,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    # Actor-Criticã®shared_encoderã®é‡ã¿ã‚’TransformerClassifierã«ã‚³ãƒ”ãƒ¼
    transformer_model.load_state_dict(actor_critic_model.shared_encoder.state_dict())
    transformer_model = transformer_model.to(device)
    
    # è©•ä¾¡ç”¨ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
    checkpoint = {
        'model_params': {
            'vocab_size': len(base_tokens),
            'pad_id': 0,
            'max_seq_len': 256,
            'd_model': model_params.d_model,
            'nhead': model_params.nhead,
            'num_layers': model_params.num_layers,
            'dim_feedforward': model_params.dim_feedforward,
            'dropout': model_params.dropout,
        },
        'vocab_size': len(base_tokens),
        'pad_id': 0,
        'max_seq_len': 256,
        'main_to_id': main_to_id,
        'arg1_to_id': arg1_to_id,
        'arg2_to_id': arg2_to_id,
        'id_to_main': id_to_main,
        'id_to_arg1': id_to_arg1,
        'id_to_arg2': id_to_arg2,
    }
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’è¿½åŠ 
    checkpoint.update(transformer_model.state_dict())
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    torch.save(checkpoint, output_path)
    
    print(f"âœ… TransformerClassifier extracted and saved to: {output_path}")
    print(f"ğŸ“Š Model info:")
    print(f"  Vocab size: {len(base_tokens)}")
    print(f"  Main tactics: {len(id_to_main)}")
    print(f"  Arg1 values: {len(id_to_arg1)}")
    print(f"  Arg2 values: {len(id_to_arg2)}")


def main():
    parser = argparse.ArgumentParser(description="Extract TransformerClassifier from Actor-Critic model")
    parser.add_argument("--actor_critic_path", type=str, required=True, help="Actor-Critic model path")
    parser.add_argument("--output_path", type=str, required=True, help="Output path for TransformerClassifier")
    parser.add_argument("--pretrained_model", type=str, default="models/pretrained_model.pth", help="Pretrained model path")
    
    args = parser.parse_args()
    
    extract_transformer_from_actor_critic(
        actor_critic_path=args.actor_critic_path,
        output_path=args.output_path,
        pretrained_model_path=args.pretrained_model
    )


if __name__ == "__main__":
    main()
