# FOF (First-Order Formula) - Transformer-based Theorem Prover

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Transformerãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦å‘½é¡Œè«–ç†ã®å®šç†è¨¼æ˜ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚[pyprover](https://github.com/kaicho8636/pyprover)ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨çµ„ã¿åˆã‚ã›ã¦ã€æ•°å¼ç”Ÿæˆã‹ã‚‰è¨¼æ˜æˆ¦ç•¥ã®äºˆæ¸¬ã¾ã§ä¸€è²«ã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸš€ ä¸»ãªç‰¹å¾´

- **éšå±¤åˆ†é¡ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ã®ç¨®é¡ã¨å¼•æ•°ã‚’ç‹¬ç«‹ã—ã¦äºˆæ¸¬
- **æ¨è«–æ€§èƒ½è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ **: å®Ÿéš›ã®å•é¡Œè§£æ±ºèƒ½åŠ›ã‚’æ¸¬å®šã™ã‚‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
- **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†**: GCSçµ±åˆã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ç®¡ç†
- **é«˜åº¦ãªé‡è¤‡æ’é™¤**: åŠ¹ç‡çš„ãªé‡è¤‡æ’é™¤ã‚·ã‚¹ãƒ†ãƒ 
- **ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†**: ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å¯¾å¿œã®é«˜é€Ÿãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- **å®Ÿé¨“è¿½è·¡**: wandbã«ã‚ˆã‚‹è©³ç´°ãªå­¦ç¿’ãƒ»æ¨è«–ãƒ­ã‚°

## ç’°å¢ƒè¨­å®š

### ä»®æƒ³ç’°å¢ƒã®ä½œæˆã¨æœ‰åŠ¹åŒ–

```bash
# ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python -m venv .venv

# ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
# macOS/Linux:
source .venv/bin/activate
# Windows:
# .venv\Scripts\activate

# ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

### ä¾å­˜é–¢ä¿‚

- Python 3.8+
- PyTorch
- [pyprover](https://github.com/kaicho8636/pyprover) - å‘½é¡Œè«–ç†è¨¼æ˜å™¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- wandb (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) - å®Ÿé¨“è¿½è·¡
- ãã®ä»–ã®ä¾å­˜é–¢ä¿‚ã¯ `requirements.txt` ã‚’å‚ç…§

### pyproverã«ã¤ã„ã¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯[pyprover](https://github.com/kaicho8636/pyprover)ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦å‘½é¡Œè«–ç†ã®å®šç†è¨¼æ˜ã‚’è¡Œã„ã¾ã™ã€‚pyproverã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¡ã¾ã™ï¼š

- **Coqãƒ©ã‚¤ã‚¯ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**
- **å¤å…¸è«–ç†ã®ã‚µãƒãƒ¼ãƒˆ**
- **ç›´æ„Ÿçš„ãªæˆ¦ç•¥ã‚·ã‚¹ãƒ†ãƒ **ï¼ˆassumption, intro, apply, split, left, right, destruct, specialize, add_dn, autoï¼‰
- **å‘½é¡Œè¨˜å·ã®ã‚µãƒãƒ¼ãƒˆ**ï¼ˆâ†’, âˆ§, âˆ¨, Â¬ï¼‰

pyproverã®è©³ç´°ãªä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/kaicho8636/pyprover)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
FOF/
â”œâ”€â”€ src/                          # ãƒ¡ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ interaction/              # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼‰
â”‚   â”‚   â””â”€â”€ run_interaction.py    # ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã€‚æ•°å¼ç”Ÿæˆã€Transformeräºˆæ¸¬ã€è¨¼æ˜å®Ÿè¡Œã®çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
â”‚   â”œâ”€â”€ data_generation/          # äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ auto_data_collector.py        # auto_classical()ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”‚   â””â”€â”€ auto_data_parallel_collector.py # ä¸¦åˆ—å‡¦ç†å¯¾å¿œã®é«˜é€Ÿãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆGCSçµ±åˆï¼‰
â”‚   â”œâ”€â”€ training/                 # å­¦ç¿’é–¢é€£
â”‚   â”‚   â”œâ”€â”€ train_with_generated_data.py  # ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ¨å¥¨ï¼‰
â”‚   â”‚   â”œâ”€â”€ inference_hierarchical.py     # éšå±¤åˆ†é¡å¯¾å¿œã®æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”‚   â”œâ”€â”€ analyze_generated_data.py     # ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
â”‚   â”‚   â”œâ”€â”€ check_duplicates.py           # é‡è¤‡ãƒã‚§ãƒƒã‚¯
â”‚   â”‚   â”œâ”€â”€ deduplicate_generated_data.py # ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤
â”‚   â”‚   â”œâ”€â”€ deduplicate_gcs_data.py       # GCSãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤
â”‚   â”‚   â”œâ”€â”€ run_training.py               # å­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”‚   â””â”€â”€ README.md                     # å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â”‚   â”œâ”€â”€ compression/              # åœ§ç¸®é–¢é€£
â”‚   â”‚   â”œâ”€â”€ create_compressed_training_data.py # åœ§ç¸®ã•ã‚ŒãŸã‚¿ã‚¯ãƒ†ã‚£ã‚¯ã§æ–°ã—ã„training_data.jsonã‚’ä½œæˆ
â”‚   â”‚   â””â”€â”€ extract_tactics.py        # BPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åœ§ç¸®
â”‚   â””â”€â”€ core/                     # ã‚³ã‚¢æ©Ÿèƒ½ï¼ˆå…±é€šï¼‰
â”‚       â”œâ”€â”€ transformer_classifier.py # Transformerãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…
â”‚       â”œâ”€â”€ state_encoder.py         # è¨¼æ˜çŠ¶æ…‹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
â”‚       â”œâ”€â”€ generate_prop.py         # å‘½é¡Œè«–ç†å¼ã®ç”Ÿæˆå™¨
â”‚       â”œâ”€â”€ parameter.py             # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†
â”‚       â”œâ”€â”€ utils.py                 # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
â”‚       â””â”€â”€ fof_tokens.py            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã¨å‡ºåŠ›ãƒ©ãƒ™ãƒ«ã®å®šç¾©
â”œâ”€â”€ tests/                        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ test_parameter_sync.py    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_tactic_tokens.py     # ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_integration.py       # çµ±åˆãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_wandb_connection.py  # wandbæ¥ç¶šãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_single_file_training.py # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_duplicate_check.py   # é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_deduplicated_data_hashes.py # é‡è¤‡æ’é™¤ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_gcs_cross_file_duplicates.py # GCSã‚¯ãƒ­ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«é‡è¤‡ãƒ†ã‚¹ãƒˆ
â”‚   â””â”€â”€ debug_duplicate_counting.py # é‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒãƒƒã‚°
â”œâ”€â”€ examples/                     # ä½¿ç”¨ä¾‹
â”‚   â””â”€â”€ example_parameter_usage.py # parameter.pyã®ä½¿ç”¨ä¾‹
â”œâ”€â”€ data/                         # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ training_data.json
â”‚   â”œâ”€â”€ training_data_compressed.json
â”‚   â””â”€â”€ tactic_compression_*.json
â”œâ”€â”€ generated_data/               # ç”Ÿæˆã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ test_output_00001.json
â”‚   â”œâ”€â”€ test_output_00002.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ deduplicated_data/            # é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ deduplicated_batch_00001.json
â”‚   â”œâ”€â”€ deduplicated_batch_00002.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                       # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ hierarchical_model.pth
â”‚   â”œâ”€â”€ hierarchical_model_generated.pth
â”‚   â””â”€â”€ test_*.pth                # ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ pyprover/                     # pyproverï¼ˆæ—¢å­˜ã®ã¾ã¾ï¼‰
â”œâ”€â”€ test_inference_randomness.py  # æ¨è«–ãƒ©ãƒ³ãƒ€ãƒ æ€§ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_problem_selection.py     # å•é¡Œé¸æŠãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ deduplication_report.json     # é‡è¤‡æ’é™¤ãƒ¬ãƒãƒ¼ãƒˆ
â”œâ”€â”€ gcs_deduplication_report.json # GCSé‡è¤‡æ’é™¤ãƒ¬ãƒãƒ¼ãƒˆ
â””â”€â”€ README.md
```

## ä½¿ç”¨æ–¹æ³•

### 1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼‰

#### ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæœ¬ç•ªç’°å¢ƒæ¨å¥¨ï¼‰

```bash
# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆGCSç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
python src/data_generation/auto_data_parallel_collector.py \
  --count 1000 \
  --examples_per_file 100 \
  --workers 2 \
  --gcs_bucket fof-data-20251009-milano \
  --gcs_prefix generated_data/ \
  --dataset_file training_data

# ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ï¼‰
python src/data_generation/auto_data_parallel_collector.py --count 100 --workers 4

# é«˜é›£æ˜“åº¦ãƒ»æ·±ã„æ¢ç´¢
python src/data_generation/auto_data_parallel_collector.py \
  --count 200 \
  --difficulty 0.9 \
  --max_depth 12 \
  --workers 8 \
  --examples_per_file 50 \
  --dataset_file high_difficulty_data

# è¶…å¤§é‡ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆGCS + é«˜ä¸¦åˆ—åº¦ï¼‰
python src/data_generation/auto_data_parallel_collector.py \
  --count 10000 \
  --examples_per_file 1000 \
  --workers 16 \
  --gcs_bucket fof-data-20251009-milano \
  --gcs_prefix generated_data/ \
  --dataset_file large_scale_data \
  --buffer_size 5000
```

#### ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†ã®ç‰¹å¾´

- **ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å‡¦ç†**: CPUã‚³ã‚¢æ•°ã‚’æ´»ç”¨ã—ãŸé«˜é€Ÿãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- **GCSçµ±åˆ**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ¹ç‡çš„ãªç®¡ç†
- **é‡è¤‡æ’é™¤**: Exampleé‡è¤‡ã®è‡ªå‹•æ¤œå‡ºã¨ã‚¹ã‚­ãƒƒãƒ—
- **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- **ãƒãƒƒãƒåˆ†å‰²**: æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•åˆ†å‰²

#### åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿åé›†

```bash
# åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆTransformerä¸è¦ï¼‰
python src/data_generation/auto_data_collector.py --count 10

# æ¢ç´¢ã®æ·±ã•ã‚’èª¿æ•´
python src/data_generation/auto_data_collector.py --count 10 --max_depth 10
```

### 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’

#### ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’ï¼ˆæ¨å¥¨ï¼‰

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯**å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã«ä½¿ç”¨**ã—ã€**æ¨è«–æ€§èƒ½è©•ä¾¡**ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¸¬å®šã—ã¾ã™ã€‚å¾“æ¥ã®validationåˆ†å‰²ã¯è¡Œã‚ãšã€å®Ÿéš›ã®å•é¡Œè§£æ±ºèƒ½åŠ›ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

```bash
# åŸºæœ¬çš„ãªå­¦ç¿’ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ + æ¨è«–æ€§èƒ½è©•ä¾¡ï¼‰
python src/training/train_with_generated_data.py

# é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
python src/training/train_with_generated_data.py \
  --data_dir deduplicated_data \
  --batch_size 32 \
  --learning_rate 3e-4

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®å­¦ç¿’
python src/training/train_with_generated_data.py \
  --batch_size 64 \
  --learning_rate 1e-4 \
  --num_epochs 20 \
  --max_seq_len 512

# wandbã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’è¿½è·¡
python src/training/train_with_generated_data.py \
  --use_wandb \
  --wandb_project fof-training \
  --wandb_run_name experiment_001

# é‡è¤‡ã‚’ä¿æŒã—ãŸå­¦ç¿’
python src/training/train_with_generated_data.py --keep_duplicates

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
python src/training/train_with_generated_data.py --data_dir my_generated_data
```

#### å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³

```bash
python src/training/train_with_generated_data.py [ã‚ªãƒ—ã‚·ãƒ§ãƒ³]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  --data_dir DIR                   ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: generated_data)
  --batch_size SIZE                ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32)
  --learning_rate RATE             å­¦ç¿’ç‡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3e-4)
  --num_epochs EPOCHS              ã‚¨ãƒãƒƒã‚¯æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)
  --device DEVICE                  ãƒ‡ãƒã‚¤ã‚¹é¸æŠ auto/cpu/cuda (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: auto)
  --save_path PATH                 ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: models/hierarchical_model_generated.pth)
  --max_seq_len LEN                æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 512)
  --remove_duplicates              é‡è¤‡å‰Šé™¤ã‚’æœ‰åŠ¹åŒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
  --keep_duplicates                é‡è¤‡ã‚’ä¿æŒ (--remove_duplicatesã‚’ç„¡åŠ¹åŒ–)
  --use_wandb                      wandbã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’è¿½è·¡
  --wandb_project PROJECT          wandbãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: fof-training)
  --wandb_run_name NAME            wandbå®Ÿè¡Œå
  --arg1_loss_weight WEIGHT        arg1æå¤±é‡ã¿ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.8)
  --arg2_loss_weight WEIGHT        arg2æå¤±é‡ã¿ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.8)
  --inference_eval_examples COUNT  æ¨è«–è©•ä¾¡ä¾‹æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50)
  --inference_max_steps STEPS      æ¨è«–æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30)
  --inference_temperature TEMP     æ¨è«–æ¸©åº¦ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0)
  --validation_frequency FREQ      æ¨è«–è©•ä¾¡ã®å®Ÿè¡Œé »åº¦ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ)
  --skip_inference_eval            æ¨è«–æ€§èƒ½è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé«˜é€Ÿå­¦ç¿’ï¼‰
```

### 3. æ¨è«–å®Ÿè¡Œ

#### éšå±¤åˆ†é¡æ¨è«–

```bash
# åŸºæœ¬çš„ãªæ¨è«–
python src/training/inference_hierarchical.py

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®æ¨è«–
python src/training/inference_hierarchical.py \
  --model_path models/hierarchical_model_generated.pth \
  --num_examples 100 \
  --max_steps 20 \
  --temperature 0.8 \
  --verbose

# wandbã‚’ä½¿ç”¨ã—ãŸæ¨è«–è¿½è·¡
python src/training/inference_hierarchical.py \
  --use_wandb \
  --wandb_project fof-inference \
  --wandb_run_name inference_test
```

#### ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å®Ÿè¡Œ

```bash
# åŸºæœ¬çš„ãªå®Ÿè¡Œï¼ˆselftestï¼‰
python src/interaction/run_interaction.py --selftest

# ã‚ˆã‚Šå¤šãã®ä¾‹ã‚’ç”Ÿæˆ
python src/interaction/run_interaction.py --count 10

# é›£æ˜“åº¦ã‚’èª¿æ•´
python src/interaction/run_interaction.py --difficulty 0.7

# æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨­å®š
python src/interaction/run_interaction.py --max_steps 10
```

### 4. ãƒ‡ãƒ¼ã‚¿åˆ†æ

#### ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®åˆ†æ

```bash
# ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆåˆ†æ
python src/training/analyze_generated_data.py

# é‡è¤‡ãƒã‚§ãƒƒã‚¯
python src/training/check_duplicates.py

# ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
python src/training/analyze_generated_data.py --data_dir generated_data
```

## å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´

### å…¨ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ + æ¨è«–æ€§èƒ½è©•ä¾¡

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯å¾“æ¥ã®validationåˆ†å‰²ã‚’å»ƒæ­¢ã—ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§å®Ÿç”¨çš„ãªå­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ï¼š

#### æ–°ã—ã„å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- **å…¨ãƒ‡ãƒ¼ã‚¿å­¦ç¿’**: åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã«ä½¿ç”¨ï¼ˆãƒ‡ãƒ¼ã‚¿ã®ç„¡é§„ã‚’æ’é™¤ï¼‰
- **æ¨è«–æ€§èƒ½è©•ä¾¡**: å®Ÿéš›ã®å•é¡Œè§£æ±ºèƒ½åŠ›ã‚’æ¸¬å®šã™ã‚‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
- **ãƒ©ãƒ³ãƒ€ãƒ å•é¡Œé¸æŠ**: æ¯å›ç•°ãªã‚‹å•é¡Œã§è©•ä¾¡ï¼ˆå…¬å¹³æ€§ã®ç¢ºä¿ï¼‰
- **å®Ÿç”¨çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: æ¨è«–æˆåŠŸç‡ã¨å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§æ€§èƒ½ã‚’æ¸¬å®š

#### æ¨è«–æ€§èƒ½è©•ä¾¡ã®ç‰¹å¾´
- **æ¯å›ç•°ãªã‚‹å•é¡Œ**: ç¾åœ¨æ™‚åˆ»ã‚’ã‚·ãƒ¼ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã€æ¯å›ãƒ©ãƒ³ãƒ€ãƒ ã«å•é¡Œã‚’é¸æŠ
- **å®Ÿéš›ã®è¨¼æ˜å®Ÿè¡Œ**: pyproverã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®å®šç†è¨¼æ˜ã§æ€§èƒ½ã‚’æ¸¬å®š
- **ç´”ç²‹ãªè¨€èªãƒ¢ãƒ‡ãƒ«æ€§èƒ½**: äººå·¥çš„ãªç²¾åº¦å‘ä¸Šè¦å› ã‚’æ’é™¤ã—ãŸçœŸã®æ€§èƒ½è©•ä¾¡

### é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ

#### æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚µãƒãƒ¼ãƒˆ
- **DeduplicatedDataDataset**: é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹
- **åŠ¹ç‡çš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨**: å˜ç´”ãªstepã®é›†åˆå½¢å¼ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Š
- **ãƒãƒƒãƒå‡¦ç†**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ¹ç‡çš„ãªå‡¦ç†

#### å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³
- **é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨**: `--data_dir deduplicated_data`ã§é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®š
- **å¾“æ¥ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨**: `--data_dir generated_data`ã§å¾“æ¥ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ä½¿ç”¨
- **é‡è¤‡ä¿æŒ**: `--keep_duplicates`ã§é‡è¤‡ã‚’ä¿æŒã—ãŸå­¦ç¿’

### éšå±¤åˆ†é¡ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```python
# 3ã¤ã®ç‹¬ç«‹ã—ãŸåˆ†é¡ãƒ˜ãƒƒãƒ‰
main_logits, arg1_logits, arg2_logits = model(input_ids, attention_mask)

# ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ã®ç¨®é¡ã«å¿œã˜ãŸå¼•æ•°è¦ä»¶
TACTIC_ARG_MASK = {
    "intro": (False, False),      # å¼•æ•°ä¸è¦
    "apply": (True, False),       # arg1ã®ã¿å¿…è¦
    "specialize": (True, True),   # arg1, arg2ä¸¡æ–¹å¿…è¦
    # ...
}
```

### ãƒ‡ãƒ¼ã‚¿å½¢å¼

#### å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å½¢å¼
```json
[
  {
    "example_hash": "1743dfaf9bb101e51276719e50ba05ce",
    "meta": {
      "goal_original": "((((b âˆ§ a) âˆ§ ((b âˆ¨ c) â†’ False)) âˆ§ (c â†’ (b â†’ c))) â†’ b)",
      "is_proved": true
    },
    "steps": [
      {
        "step_index": 0,
        "premises": [],
        "goal": "((((b âˆ§ a) âˆ§ ((b âˆ¨ c) â†’ False)) âˆ§ (c â†’ (b â†’ c))) â†’ b)",
        "tactic": {
          "main": "intro",
          "arg1": null,
          "arg2": null
        },
        "tactic_apply": true,
        "state_hash": "29326faf43695967bc47255fc73a580c"
      }
    ]
  }
]
```

#### å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
```
[CLS] Goal [SEP] Premiseâ‚ [SEP] Premiseâ‚‚ [SEP] Premiseâ‚ƒ [SEP] ... [EOS]
```

## å®Ÿé¨“è¿½è·¡ï¼ˆwandbï¼‰

### å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹

å­¦ç¿’ä¸­ã«ä»¥ä¸‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¨˜éŒ²ã•ã‚Œã¾ã™ï¼š

- `train_loss`: è¨“ç·´æå¤±
- `inference/success_rate`: æ¨è«–æˆåŠŸç‡ï¼ˆå®Ÿéš›ã®å•é¡Œè§£æ±ºèƒ½åŠ›ï¼‰
- `inference/avg_steps`: æ¨è«–å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆè§£æ±ºæ™‚ã®åŠ¹ç‡æ€§ï¼‰
- `learning_rate`: å­¦ç¿’ç‡
- `best_inference_success_rate`: æœ€é«˜æ¨è«–æˆåŠŸç‡ï¼ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ã®åŸºæº–ï¼‰

#### æ¨è«–æ€§èƒ½è©•ä¾¡ã®è©³ç´°
- **å•é¡Œé¸æŠ**: æ¯å›ãƒ©ãƒ³ãƒ€ãƒ ã«ç•°ãªã‚‹å•é¡Œã‚’é¸æŠï¼ˆå…¬å¹³æ€§ç¢ºä¿ï¼‰
- **è©•ä¾¡é »åº¦**: æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã”ã¨ã«å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰
- **è©•ä¾¡ä¾‹æ•°**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50ä¾‹ï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰
- **æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°**: å„å•é¡Œã®æœ€å¤§è¨¼æ˜ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30ï¼‰

### ä½¿ç”¨æ–¹æ³•

```bash
# wandbã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’
python src/training/train_with_generated_data.py --use_wandb

# ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
python src/training/train_with_generated_data.py \
  --use_wandb \
  --wandb_project my-fof-experiment \
  --wandb_run_name run_001
```

## é‡è¤‡æ’é™¤ã‚·ã‚¹ãƒ†ãƒ 

### æ¦‚è¦

å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ¹ç‡çš„ãªå‡¦ç†ã®ãŸã‚ã€é‡è¤‡æ’é™¤æ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å“è³ªå‘ä¸Šã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### é‡è¤‡æ’é™¤ã®å®Ÿè¡Œ

#### ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤

```bash
# ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤
python src/training/deduplicate_generated_data.py \
    --input_dir generated_data \
    --output_dir deduplicated_data \
    --report_file deduplication_report.json \
    --verbose
```

#### GCSãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰

```bash
# GCSãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤
python src/training/deduplicate_gcs_data.py \
    --gcs_bucket fof-data-20251009-milano \
    --gcs_prefix generated_data/ \
    --output_dir deduplicated_data \
    --report_file gcs_deduplication_report.json \
    --batch_size 20000 \
    --max_workers 8 \
    --verbose
```

### é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä½¿ç”¨

é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ï¼š

```bash
# é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
python src/training/train_with_generated_data.py \
    --data_dir deduplicated_data \
    --batch_size 32 \
    --learning_rate 3e-4

# ã¾ãŸã¯å¾“æ¥é€šã‚Šï¼ˆé‡è¤‡æ’é™¤ã‚’å­¦ç¿’æ™‚ã«å®Ÿè¡Œï¼‰
python src/training/train_with_generated_data.py \
    --data_dir generated_data \
    --batch_size 32 \
    --learning_rate 3e-4
```

### é‡è¤‡æ’é™¤ã®ç¨®é¡

1. **Exampleé‡è¤‡**: åŒã˜`example_hash`ã®ä¾‹ï¼ˆå®Œå…¨ã«åŒä¸€ã®å•é¡Œï¼‰
2. **Stateé‡è¤‡**: åŒã˜`state_hash`ã®ä¾‹ï¼ˆåŒã˜è¨¼æ˜çŠ¶æ…‹ï¼‰
3. **State-Tacticé‡è¤‡**: åŒã˜`state_tactic_hash`ã®ä¾‹ï¼ˆåŒã˜çŠ¶æ…‹+æˆ¦ç•¥ã®çµ„ã¿åˆã‚ã›ï¼‰

### å‡ºåŠ›å½¢å¼

é‡è¤‡æ’é™¤å¾Œã¯è¨¼æ˜ã®é€£ç¶šæ€§ãŒå¤±ã‚ã‚Œã‚‹ãŸã‚ã€å˜ç´”ãªstepã®é›†åˆã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ï¼š

```json
[
  {
    "step_index": 0,
    "premises": [],
    "goal": "((((b âˆ§ a) âˆ§ ((b âˆ¨ c) â†’ False)) âˆ§ (c â†’ (b â†’ c))) â†’ b)",
    "tactic": {
      "main": "intro",
      "arg1": null,
      "arg2": null
    },
    "tactic_apply": true,
    "state_hash": "29326faf43695967bc47255fc73a580c"
  }
]
```

### GCSçµ±åˆ

å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ¹ç‡çš„ãªå‡¦ç†ã®ãŸã‚ã€Google Cloud Storageã¨ã®çµ±åˆã‚’æä¾›ï¼š

- **ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ™‚ã«GCSã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- **ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«å‡¦ç†
- **ãƒãƒƒãƒå‡¦ç†**: æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã§ãƒãƒƒãƒåˆ†å‰²

## ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ 

### æ¦‚è¦

BPEï¼ˆByte Pair Encodingï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åœ§ç¸®ã—ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚

### åœ§ç¸®ãƒ—ãƒ­ã‚»ã‚¹

```bash
# ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®åœ§ç¸®
python src/compression/extract_tactics.py

# åœ§ç¸®ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
python src/compression/create_compressed_training_data.py
```

## æ¨è«–æ€§èƒ½è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

### æ¦‚è¦

å¾“æ¥ã®validationåˆ†å‰²ã«ä»£ã‚ã‚Šã€å®Ÿéš›ã®å•é¡Œè§£æ±ºèƒ½åŠ›ã‚’æ¸¬å®šã™ã‚‹æ¨è«–æ€§èƒ½è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

### ç‰¹å¾´

- **å…¨ãƒ‡ãƒ¼ã‚¿å­¦ç¿’**: åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã«ä½¿ç”¨
- **ãƒ©ãƒ³ãƒ€ãƒ å•é¡Œé¸æŠ**: æ¯å›ç•°ãªã‚‹å•é¡Œã§è©•ä¾¡ï¼ˆå…¬å¹³æ€§ç¢ºä¿ï¼‰
- **å®Ÿéš›ã®è¨¼æ˜å®Ÿè¡Œ**: pyproverã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®å®šç†è¨¼æ˜
- **å®Ÿç”¨çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: æ¨è«–æˆåŠŸç‡ã¨å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°

### è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹

1. **å•é¡Œé¸æŠ**: ç¾åœ¨æ™‚åˆ»ã‚’ã‚·ãƒ¼ãƒ‰ã¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ã«å•é¡Œã‚’é¸æŠ
2. **è¨¼æ˜å®Ÿè¡Œ**: é¸æŠã•ã‚ŒãŸå•é¡Œã«å¯¾ã—ã¦å®Ÿéš›ã«è¨¼æ˜ã‚’å®Ÿè¡Œ
3. **æ€§èƒ½æ¸¬å®š**: æˆåŠŸç‡ã¨å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—
4. **ãƒ¢ãƒ‡ãƒ«ä¿å­˜**: æœ€é«˜æ¨è«–æˆåŠŸç‡ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜

## ãƒ†ã‚¹ãƒˆ

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

#### åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ

```bash
# çµ±åˆãƒ†ã‚¹ãƒˆ
python tests/test_integration.py

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ
python tests/test_parameter_sync.py

# ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ
python tests/test_tactic_tokens.py

# wandbæ¥ç¶šãƒ†ã‚¹ãƒˆ
python tests/test_wandb_connection.py

# å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆ
python tests/test_single_file_training.py
```

#### é‡è¤‡æ’é™¤ãƒ†ã‚¹ãƒˆ

```bash
# é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
python tests/test_duplicate_check.py

# é‡è¤‡æ’é™¤ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ
python tests/test_deduplicated_data_hashes.py

# GCSã‚¯ãƒ­ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«é‡è¤‡ãƒ†ã‚¹ãƒˆ
python tests/test_gcs_cross_file_duplicates.py

# é‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒãƒƒã‚°
python tests/debug_duplicate_counting.py
```

#### æ¨è«–ãƒ»å•é¡Œé¸æŠãƒ†ã‚¹ãƒˆ

```bash
# æ¨è«–ãƒ©ãƒ³ãƒ€ãƒ æ€§ãƒ†ã‚¹ãƒˆ
python test_inference_randomness.py

# å•é¡Œé¸æŠãƒ†ã‚¹ãƒˆ
python test_problem_selection.py
```

### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸

- **åŸºæœ¬æ©Ÿèƒ½**: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
- **å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ **: å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å­¦ç¿’ã€wandbçµ±åˆ
- **é‡è¤‡æ’é™¤**: å„ç¨®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã€ãƒãƒƒã‚·ãƒ¥æ¤œè¨¼
- **GCSçµ±åˆ**: ã‚¯ãƒ­ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«é‡è¤‡æ¤œå‡º
- **æ¨è«–ã‚·ã‚¹ãƒ†ãƒ **: ãƒ©ãƒ³ãƒ€ãƒ æ€§ã€å•é¡Œé¸æŠã®å…¬å¹³æ€§

## è¨¼æ˜æˆ¦ç•¥

ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®æˆ¦ç•¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ï¼š

| æˆ¦ç•¥ | main | arg1 | arg2 | èª¬æ˜ |
|------|------|------|------|------|
| `assumption` | "assumption" | null | null | å‰æã®ç›´æ¥é©ç”¨ |
| `intro` | "intro" | null | null | å«æ„å°å…¥ |
| `split` | "split" | null | null | é€£è¨€ã®åˆ†è§£ |
| `left` | "left" | null | null | é¸è¨€ã®å·¦å´é¸æŠ |
| `right` | "right" | null | null | é¸è¨€ã®å³å´é¸æŠ |
| `add_dn` | "add_dn" | null | null | äºŒé‡å¦å®šã®è¿½åŠ  |
| `apply N` | "apply" | "N" | null | å‰æNã®é©ç”¨ |
| `destruct N` | "destruct" | "N" | null | å‰æNã®åˆ†è§£ |
| `specialize N M` | "specialize" | "N" | "M" | å‰æNã‚’Mã§ç‰¹æ®ŠåŒ– |

## é–‹ç™º

### ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„

- `src/core/transformer_classifier.py`ã§ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’èª¿æ•´
- `src/core/state_encoder.py`ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–¹æ³•ã‚’èª¿æ•´
- `src/core/parameter.py`ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç®¡ç†

### æ–°ã—ã„å…¥åŠ›å½¢å¼ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

- `src/core/transformer_classifier.py`ã®`encode()`ãƒ¡ã‚½ãƒƒãƒ‰ã§å…¥åŠ›å½¢å¼ã‚’èª¿æ•´
- `src/core/state_encoder.py`ã®`encode_prover_state()`ã§çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª¿æ•´
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆIDã®å‰²ã‚Šå½“ã¦ï¼ˆ0=special, 1=goal, 2+=premisesï¼‰ã‚’å¤‰æ›´å¯èƒ½

## æ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰

```bash
# 1. å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆGCSç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
python src/data_generation/auto_data_parallel_collector.py \
  --count 10000 \
  --examples_per_file 1000 \
  --workers 8 \
  --gcs_bucket fof-data-20251009-milano \
  --gcs_prefix generated_data/ \
  --dataset_file large_scale_data

# 2. GCSãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤
python src/training/deduplicate_gcs_data.py \
  --gcs_bucket fof-data-20251009-milano \
  --gcs_prefix generated_data/ \
  --output_dir deduplicated_data \
  --report_file gcs_deduplication_report.json \
  --batch_size 20000 \
  --max_workers 8 \
  --verbose

# 3. é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
python src/training/train_with_generated_data.py \
  --data_dir deduplicated_data \
  --batch_size 32 \
  --learning_rate 3e-4 \
  --num_epochs 10 \
  --use_wandb \
  --wandb_project fof-training \
  --wandb_run_name large_scale_experiment

# 4. æ¨è«–æ€§èƒ½è©•ä¾¡
python src/training/inference_hierarchical.py \
  --model_path models/hierarchical_model_generated.pth \
  --num_examples 100 \
  --max_steps 30 \
  --use_wandb \
  --wandb_project fof-inference
```

### ç°¡æ˜“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰

```bash
# 1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
python src/data_generation/auto_data_parallel_collector.py --count 100 --workers 4

# 2. é‡è¤‡æ’é™¤
python src/training/deduplicate_generated_data.py \
  --input_dir generated_data \
  --output_dir deduplicated_data \
  --verbose

# 3. å­¦ç¿’
python src/training/train_with_generated_data.py \
  --data_dir deduplicated_data \
  --use_wandb

# 4. æ¨è«–
python src/training/inference_hierarchical.py --verbose
```

### é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```bash
# 1. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python tests/test_integration.py
python tests/test_duplicate_check.py
python tests/test_wandb_connection.py

# 2. å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª
python src/data_generation/auto_data_collector.py --count 10
python src/training/train_with_generated_data.py --data_dir generated_data --num_epochs 1

# 3. æ¨è«–ãƒ†ã‚¹ãƒˆ
python test_inference_randomness.py
python test_problem_selection.py
```

## è¬è¾

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä»¥ä¸‹ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ï¼š

- **[pyprover](https://github.com/kaicho8636/pyprover)** - å‘½é¡Œè«–ç†è¨¼æ˜å™¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **PyTorch** - æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **wandb** - å®Ÿé¨“è¿½è·¡ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 