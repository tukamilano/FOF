#!/usr/bin/env python3
"""
Actor-Criticå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
trial1ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰pretrained_modelã‚’ä½¿ã£ã¦åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""
import os
import sys
import json
import torch
import argparse
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(__file__)
sys.path.insert(0, project_root)

from src.core.transformer_classifier import (
    load_tokens_and_labels_from_token_py,
    CharTokenizer,
    TransformerClassifier,
    build_hierarchical_label_mappings,
)
from src.core.parameter import get_model_params, get_hierarchical_labels
from src.interaction.self_improvement_data_collector import collect_comprehensive_rl_data


def load_trial1_data(trial1_dir: str = "tautology/trial1", max_files: int = None) -> list:
    """
    trial1ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è«–ç†å¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        trial1_dir: trial1ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        max_files: èª­ã¿è¾¼ã‚€æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
    
    Returns:
        è«–ç†å¼ã®ãƒªã‚¹ãƒˆ
    """
    trial1_path = Path(trial1_dir)
    if not trial1_path.exists():
        print(f"âŒ Trial1 directory not found: {trial1_dir}")
        return []
    
    json_files = sorted(trial1_path.glob("tautology_data_*.json"))
    if max_files:
        json_files = json_files[:max_files]
    
    print(f"ğŸ“ Found {len(json_files)} JSON files in {trial1_dir}")
    
    all_formulas = []
    for json_file in json_files:
        print(f"ğŸ“– Loading {json_file.name}...")
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                formulas = json.load(f)
            
            if isinstance(formulas, list):
                all_formulas.extend(formulas)
                print(f"   Loaded {len(formulas)} formulas")
            else:
                print(f"   Warning: {json_file.name} is not a list format")
                
        except Exception as e:
            print(f"   Error loading {json_file.name}: {e}")
            continue
    
    print(f"ğŸ“Š Total formulas loaded: {len(all_formulas)}")
    return all_formulas


def save_trial1_data_as_generated_data(formulas: list, output_dir: str = "generated_data_trial1"):
    """
    trial1ãƒ‡ãƒ¼ã‚¿ã‚’generated_dataå½¢å¼ã§ä¿å­˜
    
    Args:
        formulas: è«–ç†å¼ã®ãƒªã‚¹ãƒˆ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²ã—ã¦ä¿å­˜
    batch_size = 1000
    for i in range(0, len(formulas), batch_size):
        batch_formulas = formulas[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        filename = f"tautology_data_{batch_num:05d}.json"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(batch_formulas, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Saved {len(batch_formulas)} formulas to {filename}")
    
    print(f"âœ… All formulas saved to {output_dir}")


def generate_actor_critic_data(
    pretrained_model_path: str = "models/pretrained_model.pth",
    trial1_dir: str = "tautology/trial1",
    output_dir: str = "actor_critic_data",
    max_trial1_files: int = 10,  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘ãªã‚
    num_examples: int = 100,
    max_steps: int = 20,
    temperature: float = 1.0,
    success_reward: float = 1.0,
    step_penalty: float = 0.01,
    failure_penalty: float = -0.1,
    device: str = "auto"
):
    """
    Actor-Criticå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        pretrained_model_path: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        trial1_dir: trial1ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        max_trial1_files: ä½¿ç”¨ã™ã‚‹trial1ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        num_examples: å‡¦ç†ã™ã‚‹ä¾‹æ•°
        max_steps: æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        success_reward: æˆåŠŸæ™‚ã®å ±é…¬
        step_penalty: ã‚¹ãƒ†ãƒƒãƒ—ãƒšãƒŠãƒ«ãƒ†ã‚£
        failure_penalty: å¤±æ•—æ™‚ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        device: ãƒ‡ãƒã‚¤ã‚¹
    """
    print("ğŸš€ Starting Actor-Critic data generation...")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    if device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device)
    print(f"Using device: {device}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    model_params = get_model_params()
    hierarchical_labels = get_hierarchical_labels()
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿
    token_py_path = os.path.join(project_root, "src", "core", "fof_tokens.py")
    base_tokens, _ = load_tokens_and_labels_from_token_py(token_py_path)
    
    # éšå±¤åˆ†é¡ç”¨ã®ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
    main_to_id, arg1_to_id, arg2_to_id, id_to_main, id_to_arg1, id_to_arg2 = build_hierarchical_label_mappings(
        hierarchical_labels.main_tactics,
        hierarchical_labels.arg1_values,
        hierarchical_labels.arg2_values
    )
    
    label_mappings = {
        'main_to_id': main_to_id,
        'arg1_to_id': arg1_to_id,
        'arg2_to_id': arg2_to_id,
        'id_to_main': id_to_main,
        'id_to_arg1': id_to_arg1,
        'id_to_arg2': id_to_arg2
    }
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    tokenizer = CharTokenizer(
        base_tokens=base_tokens,
        add_tactic_tokens=model_params.add_tactic_tokens,
        num_tactic_tokens=model_params.num_tactic_tokens
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    print("ğŸ”„ Loading pretrained model...")
    model = TransformerClassifier(
        vocab_size=tokenizer.vocab_size,
        pad_id=tokenizer.pad_id,
        max_seq_len=256,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    if os.path.exists(pretrained_model_path):
        print(f"ğŸ“¥ Loading pretrained model from: {pretrained_model_path}")
        state_dict = torch.load(pretrained_model_path, map_location=device)
        model.load_state_dict(state_dict)
        model = model.to(device)
        print("âœ… Pretrained model loaded successfully!")
    else:
        print(f"âŒ Pretrained model not found: {pretrained_model_path}")
        return
    
    # trial1ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    print(f"\nğŸ“š Loading trial1 data...")
    trial1_formulas = load_trial1_data(trial1_dir, max_trial1_files)
    
    if not trial1_formulas:
        print("âŒ No trial1 data loaded. Exiting.")
        return
    
    # trial1ãƒ‡ãƒ¼ã‚¿ã‚’generated_dataå½¢å¼ã§ä¿å­˜
    temp_generated_data_dir = "temp_generated_data_trial1"
    print(f"\nğŸ’¾ Saving trial1 data as generated_data format...")
    save_trial1_data_as_generated_data(trial1_formulas, temp_generated_data_dir)
    
    # åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿè¡Œ
    print(f"\nğŸ“Š Collecting comprehensive RL data...")
    successful_tactics, failed_tactics = collect_comprehensive_rl_data(
        model=model,
        tokenizer=tokenizer,
        label_mappings=label_mappings,
        device=device,
        max_seq_len=256,
        num_examples=num_examples,
        max_steps=max_steps,
        verbose=True,
        generated_data_dir=temp_generated_data_dir,
        temperature=temperature,
        include_failures=True,
        success_reward=success_reward,
        step_penalty=step_penalty,
        failure_penalty=failure_penalty
    )
    
    print(f"\nğŸ“ˆ Data collection completed:")
    print(f"  Successful tactics: {len(successful_tactics)}")
    print(f"  Failed tactics: {len(failed_tactics)}")
    
    # çµæœã‚’ä¿å­˜
    os.makedirs(output_dir, exist_ok=True)
    
    # æˆåŠŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    if successful_tactics:
        success_file = os.path.join(output_dir, "successful_tactics.json")
        with open(success_file, 'w', encoding='utf-8') as f:
            json.dump(successful_tactics, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Successful tactics saved to: {success_file}")
    
    # å¤±æ•—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    if failed_tactics:
        failed_file = os.path.join(output_dir, "failed_tactics.json")
        with open(failed_file, 'w', encoding='utf-8') as f:
            json.dump(failed_tactics, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Failed tactics saved to: {failed_file}")
    
    # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
    stats = {
        "total_examples_processed": num_examples,
        "successful_tactics": len(successful_tactics),
        "failed_tactics": len(failed_tactics),
        "success_rate": len(successful_tactics) / (len(successful_tactics) + len(failed_tactics)) if (len(successful_tactics) + len(failed_tactics)) > 0 else 0,
        "trial1_files_used": max_trial1_files,
        "temperature": temperature,
        "max_steps": max_steps,
        "success_reward": success_reward,
        "step_penalty": step_penalty,
        "failure_penalty": failure_penalty
    }
    
    stats_file = os.path.join(output_dir, "data_generation_stats.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“Š Statistics saved to: {stats_file}")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    import shutil
    if os.path.exists(temp_generated_data_dir):
        shutil.rmtree(temp_generated_data_dir)
        print(f"ğŸ—‘ï¸  Cleaned up temporary directory: {temp_generated_data_dir}")
    
    print(f"\nğŸ‰ Actor-Critic data generation completed!")
    print(f"ğŸ“ Output directory: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Generate Actor-Critic training data from trial1")
    parser.add_argument("--pretrained_model", type=str, default="models/pretrained_model.pth", help="pretrained model path")
    parser.add_argument("--trial1_dir", type=str, default="tautology/trial1", help="trial1 directory")
    parser.add_argument("--output_dir", type=str, default="actor_critic_data", help="output directory")
    parser.add_argument("--max_trial1_files", type=int, default=10, help="max trial1 files to use")
    parser.add_argument("--num_examples", type=int, default=100, help="number of examples to process")
    parser.add_argument("--max_steps", type=int, default=20, help="max steps per example")
    parser.add_argument("--temperature", type=float, default=1.0, help="temperature for sampling")
    parser.add_argument("--success_reward", type=float, default=1.0, help="success reward")
    parser.add_argument("--step_penalty", type=float, default=0.01, help="step penalty")
    parser.add_argument("--failure_penalty", type=float, default=-0.1, help="failure penalty")
    parser.add_argument("--device", type=str, default="auto", help="device (auto/cuda/cpu)")
    
    args = parser.parse_args()
    
    generate_actor_critic_data(
        pretrained_model_path=args.pretrained_model,
        trial1_dir=args.trial1_dir,
        output_dir=args.output_dir,
        max_trial1_files=args.max_trial1_files,
        num_examples=args.num_examples,
        max_steps=args.max_steps,
        temperature=args.temperature,
        success_reward=args.success_reward,
        step_penalty=args.step_penalty,
        failure_penalty=args.failure_penalty,
        device=args.device
    )


if __name__ == "__main__":
    main()

