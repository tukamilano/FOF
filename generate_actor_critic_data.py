#!/usr/bin/env python3
"""
Actor-Criticå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
trial1ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰pretrained_modelã‚’ä½¿ã£ã¦åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""
import os
import sys
import json
import torch
import argparse
from pathlib import Path
from google.cloud import storage

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(__file__)
sys.path.insert(0, project_root)

from src.core.transformer_classifier import (
    load_tokens_and_labels_from_token_py,
    CharTokenizer,
    TransformerClassifier,
    build_hierarchical_label_mappings,
)
from src.core.parameter import get_model_params, get_hierarchical_labels
from src.interaction.self_improvement_data_collector import collect_comprehensive_rl_data


def upload_to_gcs(local_file_path: str, gcs_bucket: str, gcs_prefix: str) -> bool:
    """Upload a local file to Google Cloud Storage.
    
    Args:
        local_file_path: Path to the local file to upload
        gcs_bucket: GCS bucket name
        gcs_prefix: GCS prefix for the uploaded file
        
    Returns:
        True if upload successful, False otherwise
    """
    try:
        client = storage.Client()
        bucket = client.bucket(gcs_bucket)
        
        # Create the blob name by combining prefix and filename
        filename = os.path.basename(local_file_path)
        blob_name = f"{gcs_prefix.rstrip('/')}/{filename}" if gcs_prefix else filename
        
        blob = bucket.blob(blob_name)
        
        print(f"ğŸ“¤ Uploading {local_file_path} to gs://{gcs_bucket}/{blob_name}")
        blob.upload_from_filename(local_file_path)
        
        print(f"âœ… Successfully uploaded to gs://{gcs_bucket}/{blob_name}")
        return True
        
    except Exception as e:
        print(f"âŒ Error uploading {local_file_path} to GCS: {e}")
        return False


def save_actor_critic_data_with_gcs(
    successful_tactics: list,
    failed_tactics: list,
    output_dir: str = "actor_critic_data",
    batch_size: int = 10000,
    gcs_bucket: str = None,
    gcs_prefix: str = ""
) -> None:
    """Actor-Criticãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã€GCSã«é€æ¬¡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # æˆåŠŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒã”ã¨ã«åˆ†å‰²ã—ã¦ä¿å­˜ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if successful_tactics:
        print(f"ğŸ’¾ Saving {len(successful_tactics)} successful tactics in batches of {batch_size}...")
        for i in range(0, len(successful_tactics), batch_size):
            batch_data = successful_tactics[i:i + batch_size]
            batch_num = i // batch_size
            
            filename = f"successful_tactics_{batch_num:05d}.json"
            filepath = os.path.join(output_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Saved {len(batch_data)} successful tactics to {filepath}")
            
            # GCSã«é€æ¬¡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            if gcs_bucket:
                if upload_to_gcs(filepath, gcs_bucket, gcs_prefix):
                    print(f"âœ… Uploaded {filename} to GCS")
                else:
                    print(f"âŒ Failed to upload {filename} to GCS")
    
    # å¤±æ•—ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒã”ã¨ã«åˆ†å‰²ã—ã¦ä¿å­˜ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if failed_tactics:
        print(f"ğŸ’¾ Saving {len(failed_tactics)} failed tactics in batches of {batch_size}...")
        for i in range(0, len(failed_tactics), batch_size):
            batch_data = failed_tactics[i:i + batch_size]
            batch_num = i // batch_size
            
            filename = f"failed_tactics_{batch_num:05d}.json"
            filepath = os.path.join(output_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Saved {len(batch_data)} failed tactics to {filepath}")
            
            # GCSã«é€æ¬¡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            if gcs_bucket:
                if upload_to_gcs(filepath, gcs_bucket, gcs_prefix):
                    print(f"âœ… Uploaded {filename} to GCS")
                else:
                    print(f"âŒ Failed to upload {filename} to GCS")


def load_trial1_data(trial1_dir: str = "tautology/trial1", max_files: int = None) -> list:
    """
    trial1ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è«–ç†å¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        trial1_dir: trial1ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        max_files: èª­ã¿è¾¼ã‚€æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
    
    Returns:
        è«–ç†å¼ã®ãƒªã‚¹ãƒˆ
    """
    trial1_path = Path(trial1_dir)
    if not trial1_path.exists():
        print(f"âŒ Trial1 directory not found: {trial1_dir}")
        return []
    
    json_files = sorted(trial1_path.glob("tautology_data_*.json"))
    if max_files:
        json_files = json_files[:max_files]
    
    print(f"ğŸ“ Found {len(json_files)} JSON files in {trial1_dir}")
    
    all_formulas = []
    for json_file in json_files:
        print(f"ğŸ“– Loading {json_file.name}...")
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                formulas = json.load(f)
            
            if isinstance(formulas, list):
                all_formulas.extend(formulas)
                print(f"   Loaded {len(formulas)} formulas")
            else:
                print(f"   Warning: {json_file.name} is not a list format")
                
        except Exception as e:
            print(f"   Error loading {json_file.name}: {e}")
            continue
    
    print(f"ğŸ“Š Total formulas loaded: {len(all_formulas)}")
    return all_formulas


def save_trial1_data_as_generated_data(formulas: list, output_dir: str = "generated_data_trial1"):
    """
    trial1ãƒ‡ãƒ¼ã‚¿ã‚’generated_dataå½¢å¼ã§ä¿å­˜
    
    Args:
        formulas: è«–ç†å¼ã®ãƒªã‚¹ãƒˆ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²ã—ã¦ä¿å­˜
    batch_size = 1000
    for i in range(0, len(formulas), batch_size):
        batch_formulas = formulas[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        filename = f"tautology_data_{batch_num:05d}.json"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(batch_formulas, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Saved {len(batch_formulas)} formulas to {filename}")
    
    print(f"âœ… All formulas saved to {output_dir}")


def generate_actor_critic_data(
    pretrained_model_path: str = "models/pretrained_model.pth",
    trial1_dir: str = "tautology/trial1",
    output_dir: str = "actor_critic_data",
    max_trial1_files: int = 10,  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘ãªã‚
    num_examples: int = 100,
    max_steps: int = 20,
    temperature: float = 1.0,
    success_reward: float = 1.0,
    step_penalty: float = 0.01,
    failure_penalty: float = -0.1,
    device: str = "auto",
    gcs_bucket: str = None,
    gcs_prefix: str = "",
    batch_size: int = 10000
):
    """
    Actor-Criticå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        pretrained_model_path: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        trial1_dir: trial1ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        max_trial1_files: ä½¿ç”¨ã™ã‚‹trial1ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        num_examples: å‡¦ç†ã™ã‚‹ä¾‹æ•°
        max_steps: æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        success_reward: æˆåŠŸæ™‚ã®å ±é…¬
        step_penalty: ã‚¹ãƒ†ãƒƒãƒ—ãƒšãƒŠãƒ«ãƒ†ã‚£
        failure_penalty: å¤±æ•—æ™‚ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        device: ãƒ‡ãƒã‚¤ã‚¹
        gcs_bucket: GCSãƒã‚±ãƒƒãƒˆåï¼ˆNoneã®å ´åˆã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãªã„ï¼‰
        gcs_prefix: GCSãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10000ï¼‰
    """
    print("ğŸš€ Starting Actor-Critic data generation...")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    if device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device)
    print(f"Using device: {device}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    model_params = get_model_params()
    hierarchical_labels = get_hierarchical_labels()
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿
    token_py_path = os.path.join(project_root, "src", "core", "fof_tokens.py")
    base_tokens, _ = load_tokens_and_labels_from_token_py(token_py_path)
    
    # éšå±¤åˆ†é¡ç”¨ã®ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
    main_to_id, arg1_to_id, arg2_to_id, id_to_main, id_to_arg1, id_to_arg2 = build_hierarchical_label_mappings(
        hierarchical_labels.main_tactics,
        hierarchical_labels.arg1_values,
        hierarchical_labels.arg2_values
    )
    
    label_mappings = {
        'main_to_id': main_to_id,
        'arg1_to_id': arg1_to_id,
        'arg2_to_id': arg2_to_id,
        'id_to_main': id_to_main,
        'id_to_arg1': id_to_arg1,
        'id_to_arg2': id_to_arg2
    }
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    tokenizer = CharTokenizer(
        base_tokens=base_tokens,
        add_tactic_tokens=model_params.add_tactic_tokens,
        num_tactic_tokens=model_params.num_tactic_tokens
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    print("ğŸ”„ Loading pretrained model...")
    model = TransformerClassifier(
        vocab_size=tokenizer.vocab_size,
        pad_id=tokenizer.pad_id,
        max_seq_len=256,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    if os.path.exists(pretrained_model_path):
        print(f"ğŸ“¥ Loading pretrained model from: {pretrained_model_path}")
        state_dict = torch.load(pretrained_model_path, map_location=device)
        model.load_state_dict(state_dict)
        model = model.to(device)
        print("âœ… Pretrained model loaded successfully!")
    else:
        print(f"âŒ Pretrained model not found: {pretrained_model_path}")
        return
    
    # trial1ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    print(f"\nğŸ“š Loading trial1 data...")
    trial1_formulas = load_trial1_data(trial1_dir, max_trial1_files)
    
    if not trial1_formulas:
        print("âŒ No trial1 data loaded. Exiting.")
        return
    
    # trial1ãƒ‡ãƒ¼ã‚¿ã‚’generated_dataå½¢å¼ã§ä¿å­˜
    temp_generated_data_dir = "temp_generated_data_trial1"
    print(f"\nğŸ’¾ Saving trial1 data as generated_data format...")
    save_trial1_data_as_generated_data(trial1_formulas, temp_generated_data_dir)
    
    # åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿè¡Œ
    print(f"\nğŸ“Š Collecting comprehensive RL data...")
    successful_tactics, failed_tactics = collect_comprehensive_rl_data(
        model=model,
        tokenizer=tokenizer,
        label_mappings=label_mappings,
        device=device,
        max_seq_len=256,
        num_examples=num_examples,
        max_steps=max_steps,
        verbose=True,
        generated_data_dir=temp_generated_data_dir,
        temperature=temperature,
        include_failures=True,
        success_reward=success_reward,
        step_penalty=step_penalty,
        failure_penalty=failure_penalty
    )
    
    print(f"\nğŸ“ˆ Data collection completed:")
    print(f"  Successful tactics: {len(successful_tactics)}")
    print(f"  Failed tactics: {len(failed_tactics)}")
    
    # çµæœã‚’ä¿å­˜ï¼ˆGCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’ä½¿ç”¨ï¼‰
    save_actor_critic_data_with_gcs(
        successful_tactics=successful_tactics,
        failed_tactics=failed_tactics,
        output_dir=output_dir,
        batch_size=batch_size,
        gcs_bucket=gcs_bucket,
        gcs_prefix=gcs_prefix
    )
    
    # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
    stats = {
        "total_examples_processed": num_examples,
        "successful_tactics": len(successful_tactics),
        "failed_tactics": len(failed_tactics),
        "success_rate": len(successful_tactics) / (len(successful_tactics) + len(failed_tactics)) if (len(successful_tactics) + len(failed_tactics)) > 0 else 0,
        "trial1_files_used": max_trial1_files,
        "temperature": temperature,
        "max_steps": max_steps,
        "success_reward": success_reward,
        "step_penalty": step_penalty,
        "failure_penalty": failure_penalty
    }
    
    stats_file = os.path.join(output_dir, "data_generation_stats.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“Š Statistics saved to: {stats_file}")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    import shutil
    if os.path.exists(temp_generated_data_dir):
        shutil.rmtree(temp_generated_data_dir)
        print(f"ğŸ—‘ï¸  Cleaned up temporary directory: {temp_generated_data_dir}")
    
    print(f"\nğŸ‰ Actor-Critic data generation completed!")
    print(f"ğŸ“ Output directory: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Generate Actor-Critic training data from trial1")
    parser.add_argument("--pretrained_model", type=str, default="models/pretrained_model.pth", help="pretrained model path")
    parser.add_argument("--trial1_dir", type=str, default="tautology/trial1", help="trial1 directory")
    parser.add_argument("--output_dir", type=str, default="actor_critic_data", help="output directory")
    parser.add_argument("--max_trial1_files", type=int, default=10, help="max trial1 files to use")
    parser.add_argument("--num_examples", type=int, default=100, help="number of examples to process")
    parser.add_argument("--max_steps", type=int, default=20, help="max steps per example")
    parser.add_argument("--temperature", type=float, default=1.0, help="temperature for sampling")
    parser.add_argument("--success_reward", type=float, default=1.0, help="success reward")
    parser.add_argument("--step_penalty", type=float, default=0.01, help="step penalty")
    parser.add_argument("--failure_penalty", type=float, default=-0.1, help="failure penalty")
    parser.add_argument("--device", type=str, default="auto", help="device (auto/cuda/cpu)")
    parser.add_argument("--gcs_bucket", type=str, default=None, help="GCS bucket name for upload")
    parser.add_argument("--gcs_prefix", type=str, default="", help="GCS prefix for upload")
    parser.add_argument("--batch_size", type=int, default=10000, help="batch size for GCS upload")
    
    args = parser.parse_args()
    
    generate_actor_critic_data(
        pretrained_model_path=args.pretrained_model,
        trial1_dir=args.trial1_dir,
        output_dir=args.output_dir,
        max_trial1_files=args.max_trial1_files,
        num_examples=args.num_examples,
        max_steps=args.max_steps,
        temperature=args.temperature,
        success_reward=args.success_reward,
        step_penalty=args.step_penalty,
        failure_penalty=args.failure_penalty,
        device=args.device,
        gcs_bucket=args.gcs_bucket,
        gcs_prefix=args.gcs_prefix,
        batch_size=args.batch_size
    )


if __name__ == "__main__":
    main()

