from __future__ import annotations

import argparse
import os
import sys
import json
import hashlib
import time
import multiprocessing as mp
import subprocess
from typing import List, Tuple, Dict, Any, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

from src.core.generate_prop import FormulaGenerator, filter_formulas
from src.core.parameter import (
    default_params, get_generation_params, get_training_params, 
    get_system_params, DeviceType, DataFilterType
)


def example_hash(formula: str) -> str:
    """è«–ç†å¼ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
    return hashlib.md5(formula.encode()).hexdigest()


def process_single_formula_worker(args: Tuple) -> Dict[str, Any]:
    """å˜ä¸€ã®è«–ç†å¼ã‚’å‡¦ç†ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°
    
    Args:
        args: Tuple containing (formula_data, worker_id)
    
    Returns:
        Dictionary containing the processing result
    """
    formula_data, worker_id = args
    
    try:
        formula = formula_data['formula']
        
        # è«–ç†å¼ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not formula:
            return {
                'formula': '',
                'is_tautology': False,
                'worker_id': worker_id,
                'error': 'Empty formula'
            }
        
        return {
            'formula': formula,
            'is_tautology': True,  # filter_formulasã§æ—¢ã«ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ãŒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹
            'worker_id': worker_id,
            'formula_hash': example_hash(formula)
        }
        
    except Exception as e:
        return {
            'formula': formula_data.get('formula', ''),
            'is_tautology': False,
            'worker_id': worker_id,
            'error': str(e),
            'formula_hash': ''
        }


class TautologyGenerator:
    """ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ãªè«–ç†å¼ã‚’ç”Ÿæˆã—ã¦JSONã«æ ¼ç´ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 dataset_file_path: str = "tautology_data.json",
                 num_workers: int = None,
                 examples_per_file: int = 10000,
                 buffer_size: int = 1000,
                 check_duplicates: bool = True,
                 gcs_bucket: str = None,
                 gcs_prefix: str = ""):
        self.dataset_file_path = dataset_file_path
        # Default to CPU count, but allow override via num_workers parameter
        # Conservative limit of 8 to prevent memory issues
        self.num_workers = num_workers or min(mp.cpu_count(), 8)
        self.examples_per_file = examples_per_file
        self.buffer_size = min(buffer_size, examples_per_file)
        self.check_duplicates = check_duplicates
        
        # ãƒ‡ãƒ¼ã‚¿ç®¡ç†
        self.all_formulas = []
        self.current_file_index = 1
        self.formulas_in_current_file = 0
        self.buffer_formulas = 0
        self.total_generated = 0
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        self.formula_hashes = set()
        self.global_hashes_file = "global_tautology_hashes.json"
        self.load_global_hashes()
        
        # GCSè¨­å®š
        self.gcs_bucket = gcs_bucket
        self.gcs_prefix = gcs_prefix
    
    def clear_global_hashes(self):
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦ãƒªã‚»ãƒƒãƒˆ"""
        if os.path.exists(self.global_hashes_file):
            os.remove(self.global_hashes_file)
            print(f"Cleared global hashes file: {self.global_hashes_file}")
        
        # ãƒ¡ãƒ¢ãƒªå†…ã®ãƒãƒƒã‚·ãƒ¥ã‚‚ãƒªã‚»ãƒƒãƒˆ
        self.formula_hashes = set()
        self.current_file_index = 1
        print("Reset global hash state")
    
    def load_global_hashes(self):
        """æ—¢å­˜ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.global_hashes_file):
            try:
                with open(self.global_hashes_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ã‚·ãƒ³ãƒ—ãƒ«ãªå½¢å¼ï¼šãƒãƒƒã‚·ãƒ¥ã®ãƒªã‚¹ãƒˆã®ã¿
                    if isinstance(data, list):
                        self.formula_hashes = set(data)
                    else:
                        # æ—§å½¢å¼ã¨ã®äº’æ›æ€§
                        self.formula_hashes = set(data.get('formula_hashes', []))
                print(f"Loaded {len(self.formula_hashes)} existing formula hashes from {self.global_hashes_file}")
            except (FileNotFoundError, json.JSONDecodeError) as e:
                print(f"Could not load global hashes: {e}")
                self.formula_hashes = set()
        else:
            self.formula_hashes = set()
    
    def save_global_hashes(self):
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªå½¢å¼ï¼šãƒãƒƒã‚·ãƒ¥ã®ãƒªã‚¹ãƒˆã®ã¿
        with open(self.global_hashes_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.formula_hashes), f, ensure_ascii=False, indent=2)
    
    def get_current_filename(self) -> str:
        """ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—"""
        base_name = os.path.basename(self.dataset_file_path).replace('.json', '')
        return f"{base_name}_{self.current_file_index:05d}.json"
    
    def clear_generated_data(self):
        """æ—¢å­˜ã®generated_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªã‚¢"""
        generated_dir = "generated_data"
        if os.path.exists(generated_dir):
            import shutil
            shutil.rmtree(generated_dir)
            print(f"Cleared: {generated_dir}/")
        os.makedirs(generated_dir, exist_ok=True)
        
        if self.gcs_bucket:
            print(f"GCS upload: gs://{self.gcs_bucket}/{self.gcs_prefix}")
    
    def upload_file_to_gcs(self, local_file_path: str, gcs_filename: str) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSãƒã‚±ãƒƒãƒˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        if not self.gcs_bucket:
            return False
        
        try:
            # GCSãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            gcs_path = f"gs://{self.gcs_bucket}/{self.gcs_prefix}{gcs_filename}"
            
            # gcloudã‚³ãƒãƒ³ãƒ‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            result = subprocess.run([
                'gcloud', 'storage', 'cp', local_file_path, gcs_path
            ], capture_output=True, text=True, check=True)
            
            print(f"Uploaded: {gcs_filename}")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"Upload failed: {gcs_filename}")
            return False
        except Exception as e:
            print(f"Upload failed: {gcs_filename}")
            return False
    
    def generate_tautologies_parallel(self, gen, gen_params) -> List[Dict]:
        """ä¸¦åˆ—ã§ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ã‚’ç”Ÿæˆ"""
        print(f"Starting parallel tautology generation with {self.num_workers} workers...")
        
        results = []
        successful_formulas = 0
        processed_count = 0
        skipped_duplicates = 0
        batch_size = self.num_workers * 4  # Process 4x worker count at a time
        
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            with tqdm(total=gen_params.count, desc="Generating tautologies", unit="formula", 
                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}') as pbar:
                while processed_count < gen_params.count:
                    # ãƒãƒƒãƒã®è«–ç†å¼ã‚’ç”Ÿæˆ
                    batch_formulas = []
                    batch_count = min(batch_size, gen_params.count - processed_count)
                    
                    successful_count = 0
                    for i in range(batch_count):
                        # require_tautology=Trueã§ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ã®ã¿ã‚’ç”Ÿæˆ
                        goal_list = filter_formulas(gen, max_len=gen_params.max_len, require_tautology=True, limit=1)
                        if goal_list:
                            goal = goal_list[0]
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¯ä¿å­˜æ™‚ã«çµ±åˆã—ã¦è¡Œã†ãŸã‚ã€ã“ã“ã§ã¯ã‚¹ã‚­ãƒƒãƒ—
                            
                            batch_formulas.append({
                                "formula": goal,
                                "index": successful_count
                            })
                            successful_count += 1
                        else:
                            # æœ‰åŠ¹ãªè«–ç†å¼ãŒãªã„å ´åˆã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                            batch_formulas.append({
                                "formula": "",
                                "index": successful_count
                            })
                            successful_count += 1
                    
                    # ãƒãƒƒãƒã‚’ä¸¦åˆ—å‡¦ç†
                    worker_args = [
                        (formula, os.getpid()) 
                        for formula in batch_formulas
                    ]
                    
                    # ãƒãƒƒãƒã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
                    future_to_index = {
                        executor.submit(process_single_formula_worker, args): formula["index"] 
                        for args, formula in zip(worker_args, batch_formulas)
                    }
                    
                    # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
                    for future in as_completed(future_to_index):
                        index = future_to_index[future]
                        try:
                            result = future.result()
                            results.append(result)
                            
                            # æˆåŠŸã—ãŸè«–ç†å¼ã‚’åé›†
                            if result.get('is_tautology', False) and result.get('formula'):
                                self.add_formula_and_check_save(result)
                                successful_formulas += 1
                                        
                        except Exception as e:
                            results.append({
                                'formula': '',
                                'is_tautology': False,
                                'worker_id': os.getpid(),
                                'error': str(e)
                            })
                        
                        pbar.update(1)
                        processed_count += 1
                        
                        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
                        if processed_count % 100 == 0:
                            pbar.set_postfix({
                                'file': f"{self.current_file_index:05d}",
                                'formulas': len(self.all_formulas),
                                'skipped': skipped_duplicates
                            })
                        
                        # ååˆ†ã«å‡¦ç†ã—ãŸã‚‰çµ‚äº†
                        if processed_count >= gen_params.count:
                            break
                    
                    # ãƒãƒƒãƒå‡¦ç†å®Œäº†
                    
                    # ååˆ†ã«å‡¦ç†ã—ãŸã‚‰çµ‚äº†
                    if processed_count >= gen_params.count:
                        break
        
        # çµæœã‚’è«–ç†å¼ã§ã‚½ãƒ¼ãƒˆã—ã¦é †åºã‚’ç¶­æŒ
        results.sort(key=lambda x: x.get('formula', ''))
        
        print(f"Completed: {successful_formulas}/{gen_params.count} tautologies generated")
        print(f"Duplicates skipped: {skipped_duplicates} ({skipped_duplicates/gen_params.count*100:.1f}%)")
        print(f"Global unique formulas: {len(self.formula_hashes)}")
        
        return results
    
    def add_formula_and_check_save(self, formula_data: Dict):
        """è«–ç†å¼ã‚’è¿½åŠ ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶é™ã«é”ã—ãŸã‚‰ä¿å­˜"""
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¯ä¿å­˜æ™‚ã«çµ±åˆã—ã¦è¡Œã†ãŸã‚ã€ã“ã“ã§ã¯å˜ç´”ã«è¿½åŠ 
        self.all_formulas.append(formula_data)
        self.formulas_in_current_file += 1
        self.buffer_formulas += 1
        
        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ã„ãšã‚Œã‹ã«é”ã—ãŸã‚‰ä¿å­˜
        if (self.buffer_formulas >= self.buffer_size or 
            self.buffer_formulas >= self.examples_per_file):
            self.save_current_data()
    
    def save_current_data(self):
        """ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not self.all_formulas:
            return
            
        # ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
        transformed_data = self.transform_to_output_format(self.all_formulas)
        num_formulas = len(transformed_data)
        
        # ãƒãƒƒãƒ•ã‚¡ã®ä¾‹æ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.buffer_formulas = 0
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã‚’æ›´æ–°
        self.total_generated += len(self.all_formulas)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        local_file_path = self._save_to_local_file(transformed_data)
        
        # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.all_formulas = []
        self.formulas_in_current_file = 0
    
    def transform_to_output_format(self, formulas: List[Dict]) -> List[str]:
        """å‡ºåŠ›å½¢å¼ã«å¤‰æ› - è«–ç†å¼ã®æ–‡å­—åˆ—ã®ã¿ã‚’è¿”ã™"""
        transformed_data = []
        seen_hashes = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        
        for i, formula_data in enumerate(formulas):
            formula = formula_data.get('formula', '')
            if not formula:
                continue
                
            formula_hash_val = formula_data.get('formula_hash', example_hash(formula))
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if self.check_duplicates:
                if formula_hash_val in seen_hashes:
                    continue
                seen_hashes.add(formula_hash_val)
            
            transformed_data.append(formula)
        
        return transformed_data
    
    def _save_to_local_file(self, transformed_data: List[str]) -> str:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™"""
        filename = self.get_current_filename()
        num_formulas = len(transformed_data)
        
        # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        local_file_path = os.path.join("generated_data", filename)
        if os.path.exists(local_file_path):
            try:
                with open(local_file_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                if len(existing_data) + num_formulas > self.examples_per_file:
                    # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæº€æ¯ã«ãªã‚‹ã®ã§ã€æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                    self.current_file_index += 1
                    filename = self.get_current_filename()
                    local_file_path = os.path.join("generated_data", filename)
                    existing_data = []
            except (FileNotFoundError, json.JSONDecodeError):
                existing_data = []
        else:
            existing_data = []
        
        # çµ±åˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ« + ãƒ•ã‚¡ã‚¤ãƒ«å†…ï¼‰
        filtered_data = []
        duplicates_removed = 0
        
        for formula in transformed_data:
            formula_hash = example_hash(formula)
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢ã«add_formula_and_check_saveã§ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ã ãŒã€å¿µã®ãŸã‚ï¼‰
            if self.check_duplicates and formula_hash in self.formula_hashes:
                duplicates_removed += 1
                continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if any(example_hash(existing_formula) == formula_hash for existing_formula in existing_data):
                duplicates_removed += 1
                continue
            
            # é‡è¤‡ãªã— - ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            filtered_data.append(formula)
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚·ãƒ¥ã«è¿½åŠ ï¼ˆã¾ã è¿½åŠ ã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
            if self.check_duplicates:
                self.formula_hashes.add(formula_hash)
        
        if duplicates_removed > 0:
            duplicate_rate = (duplicates_removed / len(transformed_data)) * 100
            print(f"  ğŸ“Š File {self.current_file_index:05d}: Removed {duplicates_removed}/{len(transformed_data)} duplicates ({duplicate_rate:.1f}%)")
        
        transformed_data = filtered_data
        num_formulas = len(transformed_data)
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        existing_data.extend(transformed_data)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        with open(local_file_path, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        # è¿½è·¡ã‚’æ›´æ–°
        self.formulas_in_current_file = len(existing_data)
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        self.save_global_hashes()
        
        # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if self.gcs_bucket:
            self.upload_file_to_gcs(local_file_path, filename)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æ™‚ã®çµ±è¨ˆã‚’è¡¨ç¤º
        if len(existing_data) == num_formulas:
            print(f"Created: {filename} ({num_formulas} formulas)")
        else:
            print(f"Appended: {filename} (+{num_formulas}, total: {len(existing_data)})")
        
        return local_file_path
    
    def save_data(self):
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        if self.all_formulas:
            self.save_current_data()
    
    def get_stats(self) -> Dict[str, int]:
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’å–å¾—"""
        total_formulas = self.total_generated + len(self.all_formulas)
        unique_formulas = len(self.formula_hashes)
        
        return {
            "total_formulas": total_formulas,
            "unique_formulas": unique_formulas,
            "files_created": self.current_file_index
        }


def main() -> None:
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    gen_params = get_generation_params()
    train_params = get_training_params()
    system_params = get_system_params()
    
    parser = argparse.ArgumentParser(description="Generate tautology formulas and save to JSON")
    parser.add_argument("--count", type=int, default=gen_params.count, help="number of formulas to generate")
    parser.add_argument("--difficulty", type=float, default=gen_params.difficulty, help="formula generation difficulty")
    parser.add_argument("--seed", type=int, default=gen_params.seed, help="random seed")
    parser.add_argument("--max_len", type=int, default=gen_params.max_len, help="maximum formula string length")
    parser.add_argument("--dataset_file", type=str, default="tautology_data", help="base name for output files")
    parser.add_argument("--workers", type=int, default=None, 
                       help="number of parallel workers (default: min(cpu_count, 8))")
    parser.add_argument("--examples_per_file", type=int, default=10000,
                       help="number of examples per output file (default: 10000)")
    parser.add_argument("--buffer_size", type=int, default=1000,
                       help="buffer size for writing data (default: 1000)")
    parser.add_argument("--gcs_bucket", type=str, default=None,
                       help="GCS bucket name for direct upload (e.g., fof-data-20251010-milano)")
    parser.add_argument("--gcs_prefix", type=str, default="tautology/",
                       help="GCS prefix for uploaded files (e.g., tautology/)")
    parser.add_argument("--keep_global_hashes", action="store_true",
                       help="Keep existing global hashes file (continue from previous run)")
    args = parser.parse_args()

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
    default_params.update_generation_params(
        count=args.count,
        difficulty=args.difficulty,
        seed=args.seed,
        max_len=args.max_len
    )
    default_params.update_training_params(
        dataset_file=args.dataset_file
    )
    
    root_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
    default_params.update_system_params(
        root_dir=root_dir,
        pyprover_dir=os.path.join(root_dir, "pyprover")
    )

    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’æ§‹ç¯‰
    gen = FormulaGenerator(
        variables=gen_params.variables, 
        allow_const=gen_params.allow_const, 
        difficulty=gen_params.difficulty, 
        seed=gen_params.seed
    )
    
    # ãƒˆãƒ¼ãƒˆãƒ­ã‚¸ãƒ¼ç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–
    tautology_generator = TautologyGenerator(
        dataset_file_path=args.dataset_file,
        num_workers=args.workers,
        examples_per_file=args.examples_per_file,
        buffer_size=args.buffer_size,
        check_duplicates=True,
        gcs_bucket=args.gcs_bucket,
        gcs_prefix=args.gcs_prefix
    )
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆ--keep_global_hashesãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    if not args.keep_global_hashes:
        tautology_generator.clear_global_hashes()

    print(f"Starting tautology generation: {gen_params.count} formulas, {tautology_generator.num_workers} workers")
    if args.gcs_bucket:
        print(f"Output: gs://{args.gcs_bucket}/{args.gcs_prefix}{args.dataset_file}_XXXXX.json")
    else:
        print(f"Output: generated_data/{args.dataset_file}_XXXXX.json")
    
    # æ—¢å­˜ã®generated_dataã‚’ã‚¯ãƒªã‚¢
    tautology_generator.clear_generated_data()
    
    start_time = time.time()
    
    try:
        # è«–ç†å¼ã‚’ä¸¦åˆ—å‡¦ç†ã§ç”Ÿæˆ
        results = tautology_generator.generate_tautologies_parallel(gen, gen_params)
        
        # æ§‹é€ åŒ–ã•ã‚ŒãŸå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        tautology_generator.save_data()
        stats = tautology_generator.get_stats()
        
        print(f"\nCompleted: {stats['total_formulas']} formulas, {stats['unique_formulas']} unique formulas")
        print(f"Files created: {stats['files_created']}")
        if args.gcs_bucket:
            print(f"Saved to: gs://{args.gcs_bucket}/{args.gcs_prefix}")
        else:
            print(f"Saved to: generated_data/")

    finally:
        # çµ‚äº†å‰ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        tautology_generator.save_global_hashes()
        
        # ç·æ™‚é–“ã‚’è¨ˆç®—
        total_time = time.time() - start_time
        print(f"Time: {total_time:.1f}s ({total_time/60:.1f}min), {total_time/gen_params.count:.2f}s/formula")


if __name__ == "__main__":
    main()
