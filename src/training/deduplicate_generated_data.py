#!/usr/bin/env python3
"""
generated_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é‡è¤‡æ’é™¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å­¦ç¿’å‰ã«é‡è¤‡ã‚’äº‹å‰ã«é™¤å»ã—ã€é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹
"""
import argparse
import json
import os
import sys
import glob
import time
from typing import List, Dict, Any, Set
from collections import Counter, defaultdict
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)


def deduplicate_generated_data(
    input_dir: str,
    output_dir: str,
    report_file: str = None,
    verbose: bool = False,
    batch_size: int = 10000
) -> Dict[str, Any]:
    """
    generated_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é‡è¤‡ã‚’é™¤å»ã—ã€é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        input_dir: å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆgenerated_dataï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆdeduplicated_dataï¼‰
        report_file: çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10000ï¼‰
    
    Returns:
        é‡è¤‡æ’é™¤çµ±è¨ˆæƒ…å ±
    """
    print(f"ğŸ” Starting deduplication process...")
    print(f"   Input directory: {input_dir}")
    print(f"   Output directory: {output_dir}")
    print(f"   Batch size: {batch_size}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    input_files = glob.glob(os.path.join(input_dir, "*.json"))
    if not input_files:
        print(f"âŒ No JSON files found in {input_dir}")
        return {}
    
    print(f"ğŸ“ Found {len(input_files)} JSON files")
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡æ’é™¤ç”¨ã®ã‚»ãƒƒãƒˆ
    seen_hashes: Set[str] = set()
    
    # ãƒãƒƒãƒå‡¦ç†ç”¨ã®å¤‰æ•°
    current_batch = []
    file_counter = 0
    
    # çµ±è¨ˆæƒ…å ±
    stats = {
        'total_files': len(input_files),
        'total_steps_before': 0,
        'total_steps_after': 0,
        'duplicate_steps': 0,
        'duplicate_rate': 0.0,
        'output_files': 0,
        'duplicate_hash_counts': Counter(),
        'processing_time': 0.0
    }
    
    start_time = time.time()
    
    def save_batch(batch_data: List[Dict], batch_num: int) -> None:
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_file = os.path.join(output_dir, f"deduplicated_batch_{batch_num:05d}.json")
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
            if verbose:
                print(f"   ğŸ’¾ Saved batch {batch_num:05d}: {len(batch_data)} steps")
        except Exception as e:
            print(f"âŒ Error saving batch {batch_num:05d}: {e}")
    
    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡æ’é™¤ã‚’å®Ÿè¡Œ
    print(f"ğŸ”„ Processing all files for global deduplication...")
    
    for file_idx, input_file in enumerate(tqdm(input_files, desc="Processing files")):
        if verbose:
            print(f"\nğŸ“„ Processing {os.path.basename(input_file)}...")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                file_data = json.load(f)
        except Exception as e:
            print(f"âŒ Error reading {input_file}: {e}")
            continue
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‡¦ç†
        for example in file_data:
            for step in example.get('steps', []):
                if step.get('tactic_apply', False):
                    stats['total_steps_before'] += 1
                    
                    state_tactic_hash = step.get('state_tactic_hash', '')
                    
                    if state_tactic_hash in seen_hashes:
                        # é‡è¤‡ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        stats['duplicate_steps'] += 1
                        stats['duplicate_hash_counts'][state_tactic_hash] += 1
                    else:
                        # æ–°ã—ã„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ 
                        seen_hashes.add(state_tactic_hash)
                        current_batch.append(step)
                        stats['total_steps_after'] += 1
                        
                        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                        if len(current_batch) >= batch_size:
                            save_batch(current_batch, file_counter)
                            file_counter += 1
                            current_batch = []
    
    # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’ä¿å­˜
    if current_batch:
        save_batch(current_batch, file_counter)
        file_counter += 1
    
    stats['output_files'] = file_counter
    
    # å…¨ä½“çµ±è¨ˆã‚’è¨ˆç®—
    stats['processing_time'] = time.time() - start_time
    stats['duplicate_rate'] = (
        stats['duplicate_steps'] / stats['total_steps_before'] * 100
        if stats['total_steps_before'] > 0 else 0.0
    )
    
    # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
    print(f"\nğŸ“Š Deduplication Summary")
    print(f"   Input files processed: {stats['total_files']}")
    print(f"   Output files created: {stats['output_files']}")
    print(f"   Total steps before: {stats['total_steps_before']}")
    print(f"   Total steps after: {stats['total_steps_after']}")
    print(f"   Duplicates removed: {stats['duplicate_steps']}")
    print(f"   Duplicate rate: {stats['duplicate_rate']:.2f}%")
    print(f"   Processing time: {stats['processing_time']:.2f}s")
    print(f"   Average steps per output file: {stats['total_steps_after'] / stats['output_files']:.0f}")
    
    # æœ€ã‚‚é‡è¤‡ãŒå¤šã„ãƒãƒƒã‚·ãƒ¥ã‚’è¡¨ç¤º
    if stats['duplicate_hash_counts']:
        print(f"\nğŸ” Top 10 Most Duplicated States:")
        for hash_val, count in stats['duplicate_hash_counts'].most_common(10):
            print(f"   Hash: {hash_val[:16]}... Count: {count}")
    
    # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    if report_file:
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“„ Statistics saved to: {report_file}")
        except Exception as e:
            print(f"âŒ Error saving report: {e}")
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="Deduplicate generated data before training",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic deduplication
  python src/training/deduplicate_generated_data.py --input_dir generated_data --output_dir deduplicated_data
  
  # With detailed report
  python src/training/deduplicate_generated_data.py \\
    --input_dir generated_data \\
    --output_dir deduplicated_data \\
    --report_file deduplication_report.json \\
    --verbose
        """
    )
    
    parser.add_argument(
        "--input_dir", 
        type=str, 
        default="generated_data",
        help="Input directory containing JSON files (default: generated_data)"
    )
    parser.add_argument(
        "--output_dir", 
        type=str, 
        default="deduplicated_data",
        help="Output directory for deduplicated files (default: deduplicated_data)"
    )
    parser.add_argument(
        "--report_file", 
        type=str, 
        default="deduplication_report.json",
        help="Path to save deduplication statistics report (default: deduplication_report.json)"
    )
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="Show detailed processing information"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=10000,
        help="Number of steps per output file (default: 10000)"
    )
    
    args = parser.parse_args()
    
    # å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(args.input_dir):
        print(f"âŒ Input directory not found: {args.input_dir}")
        sys.exit(1)
    
    # é‡è¤‡æ’é™¤ã‚’å®Ÿè¡Œ
    try:
        stats = deduplicate_generated_data(
            input_dir=args.input_dir,
            output_dir=args.output_dir,
            report_file=args.report_file,
            verbose=args.verbose,
            batch_size=args.batch_size
        )
        
        if stats:
            print(f"\nâœ… Deduplication completed successfully!")
            print(f"   Output directory: {args.output_dir}")
            if args.report_file:
                print(f"   Report file: {args.report_file}")
        else:
            print(f"âŒ Deduplication failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ Error during deduplication: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
