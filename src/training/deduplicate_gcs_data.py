#!/usr/bin/env python3
"""
GCS‰∏ä„ÅÆ„Éá„Éº„Çø„ÅÆÈáçË§áÊéíÈô§„Çπ„ÇØ„É™„Éó„Éà
state_hash from state_tactic_hash ÂÜçË®àÁÆóandÈáçË§áÊéíÈô§ ÂÆüË°å
1M„Ç™„Éº„ÉÄ„Éº„ÅÆ„Éá„Éº„Çø ÂØæÂøúdo/performÂäπÁéáÁöÑ„Å™„Ç¢„É´„Ç¥„É™„Ç∫„É† ÂÆüË£Ö
"""
import argparse
import json
import os
import sys
import glob
import time
import hashlib
from typing import List, Dict, Any, Set, Optional, Tuple
from collections import Counter, defaultdict
from tqdm import tqdm
from google.cloud import storage
import tempfile
import shutil
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import threading
from queue import Queue

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

from src.core.state_encoder import format_tactic_string


def state_tactic_hash(premises: List[str], goal: str, tactic: str) -> str:
    """
    Áä∂ÊÖã„Å®tactic„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÅÆ„Éè„ÉÉ„Ç∑„É•
    ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØ„ÇÑ„Éá„Éº„ÇøÁÆ°ÁêÜ ‰ΩøÁî®
    
    Args:
        premises: ÂâçÊèê„ÅÆ„É™„Çπ„Éà
        goal: „Ç¥„Éº„É´
        tactic: Êà¶Áï•ÊñáÂ≠óÂàó
        
    Returns:
        Áä∂ÊÖã„Å®tactic„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÅÆ„Éè„ÉÉ„Ç∑„É•ÂÄ§
    """
    record_str = f"{'|'.join(premises)}|{goal}|{tactic}"
    return hashlib.md5(record_str.encode()).hexdigest()


def download_single_file(args: Tuple) -> Optional[str]:
    """
    Âçò‰∏Ä„Éï„Ç°„Ç§„É´„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÔºà‰∏¶ÂàóÂá¶ÁêÜÁî®Ôºâ
    
    Args:
        args: (blob, local_dir) „ÅÆ„Çø„Éó„É´
        
    Returns:
        „ÉÄ„Ç¶„É≥„É≠„Éº„Éâdid„Éï„Ç°„Ç§„É´„ÅÆ„Éë„ÇπÔºàÂ§±ÊïóÊôÇ„ÅØNoneÔºâ
    """
    blob, local_dir = args
    
    try:
        # „É≠„Éº„Ç´„É´„Éï„Ç°„Ç§„É´„Éë„Çπ Generation
        local_filename = os.path.basename(blob.name)
        local_path = os.path.join(local_dir, local_filename)
        
        # „Éï„Ç°„Ç§„É´ „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
        blob.download_to_filename(local_path)
        return local_path
        
    except Exception as e:
        print(f"‚ùå Error downloading {blob.name}: {e}")
        return None


def download_gcs_data_parallel(gcs_bucket: str, gcs_prefix: str, local_dir: str, 
                              max_workers: int = 4, verbose: bool = False) -> List[str]:
    """
    GCS from „Éá„Éº„Çø ‰∏¶Âàó„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
    
    Args:
        gcs_bucket: GCS„Éê„Ç±„ÉÉ„ÉàÂêç
        gcs_prefix: GCS„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ
        local_dir: „É≠„Éº„Ç´„É´„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÂÖà„Éá„Ç£„É¨„ÇØ„Éà„É™
        max_workers: ‰∏¶Âàó„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÅÆÊúÄÂ§ß„ÉØ„Éº„Ç´„ÉºÊï∞
        verbose: Ë©≥Á¥∞„É≠„Ç∞ Ë°®Á§∫do/perform„Åã„Å©„ÅÜ„Åã
        
    Returns:
        „ÉÄ„Ç¶„É≥„É≠„Éº„Éâdid„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ„É™„Çπ„Éà
    """
    print(f"üì• Downloading data from GCS (parallel)...")
    print(f"   Bucket: {gcs_bucket}")
    print(f"   Prefix: {gcs_prefix}")
    print(f"   Local directory: {local_dir}")
    print(f"   Max workers: {max_workers}")
    
    # „É≠„Éº„Ç´„É´„Éá„Ç£„É¨„ÇØ„Éà„É™ ‰ΩúÊàê
    os.makedirs(local_dir, exist_ok=True)
    
    # GCS„ÇØ„É©„Ç§„Ç¢„É≥„Éà ÂàùÊúüÂåñ
    client = storage.Client()
    bucket = client.bucket(gcs_bucket)
    
    # „Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ „Éû„ÉÉ„ÉÅdo/perform„Éï„Ç°„Ç§„É´ get
    blobs = bucket.list_blobs(prefix=gcs_prefix)
    json_files = [blob for blob in blobs if blob.name.endswith('.json')]
    
    if not json_files:
        print(f"‚ùå No JSON files found in gs://{gcs_bucket}/{gcs_prefix}")
        return []
    
    print(f"üìÅ Found {len(json_files)} JSON files in GCS")
    
    # ‰∏¶Âàó„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ ÂÆüË°å
    downloaded_files = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Çø„Çπ„ÇØ Ê∫ñÂÇô
        download_tasks = [(blob, local_dir) for blob in json_files]
        
        # ‰∏¶Âàó„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ ÂÆüË°å
        with tqdm(total=len(download_tasks), desc="Downloading files", unit="file") as pbar:
            future_to_blob = {executor.submit(download_single_file, task): task[0] for task in download_tasks}
            
            for future in as_completed(future_to_blob):
                blob = future_to_blob[future]
                try:
                    result = future.result()
                    if result:
                        downloaded_files.append(result)
                        if verbose:
                            print(f"   ‚úÖ Downloaded: {os.path.basename(blob.name)}")
                except Exception as e:
                    print(f"‚ùå Error downloading {blob.name}: {e}")
                finally:
                    pbar.update(1)
    
    print(f"‚úÖ Downloaded {len(downloaded_files)} files")
    return downloaded_files


def download_gcs_data(gcs_bucket: str, gcs_prefix: str, local_dir: str, verbose: bool = False) -> List[str]:
    """
    GCS from „Éá„Éº„Çø „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÔºà„Ç∑„Éº„Ç±„É≥„Ç∑„É£„É´Áâà„ÄÅÂæåÊñπ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÔºâ
    
    Args:
        gcs_bucket: GCS„Éê„Ç±„ÉÉ„ÉàÂêç
        gcs_prefix: GCS„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ
        local_dir: „É≠„Éº„Ç´„É´„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÂÖà„Éá„Ç£„É¨„ÇØ„Éà„É™
        verbose: Ë©≥Á¥∞„É≠„Ç∞ Ë°®Á§∫do/perform„Åã„Å©„ÅÜ„Åã
        
    Returns:
        „ÉÄ„Ç¶„É≥„É≠„Éº„Éâdid„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ„É™„Çπ„Éà
    """
    return download_gcs_data_parallel(gcs_bucket, gcs_prefix, local_dir, max_workers=1, verbose=verbose)


def process_single_file_worker(args: Tuple) -> Dict[str, Any]:
    """
    Âçò‰∏Ä„Éï„Ç°„Ç§„É´„ÅÆÂá¶ÁêÜÔºà‰∏¶ÂàóÂá¶ÁêÜÁî®„ÄÅÈáçË§áÊéíÈô§„Å™„ÅóÔºâ
    
    Args:
        args: (file_path,) „ÅÆ„Çø„Éó„É´
        
    Returns:
        Âá¶ÁêÜÁµêÊûú„ÅÆËæûÊõ∏
    """
    file_path, = args
    
    try:
        # „Éï„Ç°„Ç§„É´ Ë™≠„ÅøËæº„Åø
        with open(file_path, 'r', encoding='utf-8') as f:
            file_data = json.load(f)
        
        processed_steps = []
        file_stats = {
            'file_name': os.path.basename(file_path),
            'total_steps': 0,
            'processing_time': 0.0
        }
        
        start_time = time.time()
        
        # „Éï„Ç°„Ç§„É´ÂÜÖ„ÅÆÂÖ®„Çπ„ÉÜ„ÉÉ„Éó Âá¶ÁêÜÔºàÈáçË§áÊéíÈô§„Å™„ÅóÔºâ
        for example in file_data:
            for step in example.get('steps', []):
                if step.get('tactic_apply', False):
                    file_stats['total_steps'] += 1
                    
                    # state_tactic_hash Generation
                    premises = step.get('premises', [])
                    goal = step.get('goal', '')
                    tactic_dict = step.get('tactic', {})
                    
                    try:
                        tactic_str = format_tactic_string(tactic_dict)
                        state_tactic_hash_val = state_tactic_hash(premises, goal, tactic_str)
                        
                        # state_tactic_hash „Çπ„ÉÜ„ÉÉ„Éó ËøΩÂä†
                        step['state_tactic_hash'] = state_tactic_hash_val
                        processed_steps.append(step)
                        
                    except Exception as e:
                        if verbose:
                            print(f"‚ö†Ô∏è Error processing step in {file_path}: {e}")
                        continue
        
        file_stats['processing_time'] = time.time() - start_time
        file_stats['processed_steps'] = processed_steps
        
        return file_stats
        
    except Exception as e:
        return {
            'file_name': os.path.basename(file_path),
            'error': str(e),
            'total_steps': 0,
            'processing_time': 0.0,
            'processed_steps': []
        }


def deduplicate_gcs_data(
    gcs_bucket: str,
    gcs_prefix: str,
    output_dir: str,
    temp_dir: Optional[str] = None,
    report_file: str = None,
    verbose: bool = False,
    batch_size: int = 10000,
    memory_efficient: bool = True,
    max_workers: int = None,
    parallel_download: bool = True
) -> Dict[str, Any]:
    """
    GCS‰∏ä„ÅÆ„Éá„Éº„Çø„ÅÆÈáçË§á Èô§Âéª„Åó„ÄÅÈáçË§áÊéíÈô§Ê∏à„Åø„Éá„Éº„Çø Generation
    
    Args:
        gcs_bucket: GCS„Éê„Ç±„ÉÉ„ÉàÂêç
        gcs_prefix: GCS„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ
        output_dir: Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™
        temp_dir: ‰∏ÄÊôÇ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Éá„Ç£„É¨„ÇØ„Éà„É™ÔºàNone„ÅÆÂ†¥Âêà„ÅØËá™ÂãïGenerationÔºâ
        report_file: Áµ±Ë®à„É¨„Éù„Éº„Éà„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ
        verbose: Ë©≥Á¥∞„É≠„Ç∞ Ë°®Á§∫do/perform„Åã„Å©„ÅÜ„Åã
        batch_size: „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫Ôºà„Éá„Éï„Ç©„É´„Éà: 10000Ôºâ
        memory_efficient: „É°„É¢„É™ÂäπÁéá„É¢„Éº„ÉâÔºà„Éá„Éï„Ç©„É´„Éà: TrueÔºâ
        max_workers: ‰∏¶ÂàóÂá¶ÁêÜ„ÅÆÊúÄÂ§ß„ÉØ„Éº„Ç´„ÉºÊï∞ÔºàNone„ÅÆÂ†¥Âêà„ÅØËá™ÂãïË®≠ÂÆöÔºâ
        parallel_download: ‰∏¶Âàó„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ ‰ΩøÁî®do/perform„Åã„Å©„ÅÜ„ÅãÔºà„Éá„Éï„Ç©„É´„Éà: TrueÔºâ
    
    Returns:
        ÈáçË§áÊéíÈô§Áµ±Ë®àÊÉÖÂ†±
    """
    print(f"üîç Starting GCS data deduplication process...")
    print(f"   GCS bucket: {gcs_bucket}")
    print(f"   GCS prefix: {gcs_prefix}")
    print(f"   Output directory: {output_dir}")
    print(f"   Batch size: {batch_size}")
    print(f"   Memory efficient: {memory_efficient}")
    print(f"   Parallel download: {parallel_download}")
    
    # ‰∏¶ÂàóÂá¶ÁêÜ„ÅÆ„ÉØ„Éº„Ç´„ÉºÊï∞ Ë®≠ÂÆö
    if max_workers is None:
        max_workers = min(mp.cpu_count(), 8)  # ÊúÄÂ§ß8„Éó„É≠„Çª„Çπ
    print(f"   Max workers: {max_workers}")
    
    # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™ ‰ΩúÊàê
    os.makedirs(output_dir, exist_ok=True)
    
    # ‰∏ÄÊôÇ„Éá„Ç£„É¨„ÇØ„Éà„É™ Ë®≠ÂÆö
    if temp_dir is None:
        temp_dir = tempfile.mkdtemp(prefix="gcs_dedup_")
        cleanup_temp = True
    else:
        os.makedirs(temp_dir, exist_ok=True)
        cleanup_temp = False
    
    try:
        # GCS from „Éá„Éº„Çø „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
        if parallel_download:
            input_files = download_gcs_data_parallel(gcs_bucket, gcs_prefix, temp_dir, 
                                                   max_workers=max_workers, verbose=verbose)
        else:
            input_files = download_gcs_data(gcs_bucket, gcs_prefix, temp_dir, verbose)
        
        if not input_files:
            print(f"‚ùå No files downloaded from GCS")
            return {}
        
        print(f"üìÅ Processing {len(input_files)} downloaded files")
        
        # „Ç∞„É≠„Éº„Éê„É´ÈáçË§áÊéíÈô§Áî®„ÅÆ„Çª„ÉÉ„ÉàÔºà„Ç∑„Éº„Ç±„É≥„Ç∑„É£„É´Âá¶ÁêÜÁî®Ôºâ
        seen_hashes: Set[str] = set()
        
        # „Éê„ÉÉ„ÉÅÂá¶ÁêÜÁî®„ÅÆÂ§âÊï∞
        current_batch = []
        file_counter = 0
        
        # Áµ±Ë®àÊÉÖÂ†±
        stats = {
            'total_files': len(input_files),
            'total_steps_before': 0,
            'total_steps_after': 0,
            'duplicate_steps': 0,
            'duplicate_rate': 0.0,
            'output_files': 0,
            'duplicate_hash_counts': Counter(),
            'processing_time': 0.0,
            'memory_usage_mb': 0.0,
            'parallel_processing': True,
            'max_workers': max_workers
        }
        
        start_time = time.time()
        
        def save_batch(batch_data: List[Dict], batch_num: int) -> None:
            """„Éê„ÉÉ„ÉÅ„Éá„Éº„Çø „Éï„Ç°„Ç§„É´ ‰øùÂ≠ò"""
            output_file = os.path.join(output_dir, f"deduplicated_batch_{batch_num:05d}.json")
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(batch_data, f, ensure_ascii=False, indent=2)
                if verbose:
                    print(f"   üíæ Saved batch {batch_num:05d}: {len(batch_data)} steps")
            except Exception as e:
                print(f"‚ùå Error saving batch {batch_num:05d}: {e}")
        
        def process_step(step: Dict) -> Optional[str]:
            """„Çπ„ÉÜ„ÉÉ„Éó from state_tactic_hash Generation"""
            try:
                premises = step.get('premises', [])
                goal = step.get('goal', '')
                tactic_dict = step.get('tactic', {})
                
                # tacticÊñáÂ≠óÂàó Generation
                tactic_str = format_tactic_string(tactic_dict)
                
                # state_tactic_hash Ë®àÁÆó
                return state_tactic_hash(premises, goal, tactic_str)
                
            except Exception as e:
                if verbose:
                    print(f"‚ö†Ô∏è Error processing step: {e}")
                return None
        
        # ‰∏¶ÂàóÂá¶ÁêÜ„É¢„Éº„Éâ„ÅÆÂÆüË£Ö
        if max_workers > 1:
            print(f"üîÑ Processing files in parallel mode with {max_workers} workers...")
            
            # ‰∏¶ÂàóÂá¶ÁêÜÁî®„ÅÆ„Çø„Çπ„ÇØ Ê∫ñÂÇô
            file_tasks = [(file_path,) for file_path in input_files]
            
            all_processed_steps = []
            
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # ‰∏¶ÂàóÂá¶ÁêÜ ÂÆüË°å
                with tqdm(total=len(file_tasks), desc="Processing files", unit="file") as pbar:
                    future_to_file = {executor.submit(process_single_file_worker, task): task[0] for task in file_tasks}
                    
                    for future in as_completed(future_to_file):
                        file_path = future_to_file[future]
                        try:
                            result = future.result()
                            
                            # Áµ±Ë®àÊÉÖÂ†± Êõ¥Êñ∞
                            stats['total_steps_before'] += result['total_steps']
                            
                            # Âá¶ÁêÜwas done„Çπ„ÉÜ„ÉÉ„Éó ÂèéÈõÜ
                            if 'processed_steps' in result:
                                all_processed_steps.extend(result['processed_steps'])
                            
                            if verbose:
                                print(f"   ‚úÖ Processed {result['file_name']}: "
                                      f"{result['total_steps']} steps "
                                      f"({result['processing_time']:.2f}s)")
                            
                        except Exception as e:
                            print(f"‚ùå Error processing {file_path}: {e}")
                        finally:
                            pbar.update(1)
            
            # „Ç∞„É≠„Éº„Éê„É´ÈáçË§áÊéíÈô§ ÂÆüË°åÔºà‰∏¶ÂàóÂá¶ÁêÜÂæåÔºâ
            print(f"üîÑ Performing global deduplication on {len(all_processed_steps)} steps...")
            global_unique_steps = []
            global_duplicate_count = 0
            
            for step in tqdm(all_processed_steps, desc="Global deduplication"):
                state_tactic_hash_val = step.get('state_tactic_hash', '')
                
                if state_tactic_hash_val in seen_hashes:
                    global_duplicate_count += 1
                    stats['duplicate_hash_counts'][state_tactic_hash_val] += 1
                else:
                    seen_hashes.add(state_tactic_hash_val)
                    global_unique_steps.append(step)
            
            # „Ç∞„É≠„Éº„Éê„É´ÈáçË§áÁµ±Ë®à Êõ¥Êñ∞
            stats['duplicate_steps'] = global_duplicate_count
            stats['total_steps_after'] = len(global_unique_steps)
            
            print(f"üíæ Saving {len(global_unique_steps)} unique steps in batches...")
            
            # „Éê„ÉÉ„ÉÅ ÂàÜÂâ≤and‰øùÂ≠òÔºàÈÄêÊ¨°Êõ∏„ÅçËæº„ÅøÔºâ
            for i in range(0, len(global_unique_steps), batch_size):
                batch = global_unique_steps[i:i + batch_size]
                save_batch(batch, file_counter)
                file_counter += 1
                
                if verbose and file_counter % 10 == 0:
                    print(f"   üíæ Saved batch {file_counter}: {len(batch)} steps")
        
        else:
            # „Ç∑„Éº„Ç±„É≥„Ç∑„É£„É´Âá¶ÁêÜ„É¢„Éº„ÉâÔºàÂÖÉ„ÅÆÂÆüË£ÖÔºâ
            print(f"üîÑ Processing files in sequential mode...")
            
            for file_idx, input_file in enumerate(tqdm(input_files, desc="Processing files")):
                if verbose:
                    print(f"\nüìÑ Processing {os.path.basename(input_file)}...")
                
                # JSON„Éï„Ç°„Ç§„É´ Ë™≠„ÅøËæº„Åø
                try:
                    with open(input_file, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                except Exception as e:
                    print(f"‚ùå Error reading {input_file}: {e}")
                    continue
                
                # „Éï„Ç°„Ç§„É´ÂÜÖ„ÅÆÂÖ®„Çπ„ÉÜ„ÉÉ„Éó Âá¶ÁêÜ
                for example in file_data:
                    for step in example.get('steps', []):
                        if step.get('tactic_apply', False):
                            stats['total_steps_before'] += 1
                            
                            # state_tactic_hash Generation
                            premises = step.get('premises', [])
                            goal = step.get('goal', '')
                            tactic_dict = step.get('tactic', {})
                            
                            try:
                                tactic_str = format_tactic_string(tactic_dict)
                                state_tactic_hash_val = state_tactic_hash(premises, goal, tactic_str)
                                
                                if state_tactic_hash_val in seen_hashes:
                                    # ÈáçË§á „Çπ„Ç≠„ÉÉ„Éó
                                    stats['duplicate_steps'] += 1
                                    stats['duplicate_hash_counts'][state_tactic_hash_val] += 1
                                else:
                                    # Êñ∞„Åó„ÅÑ„Çπ„ÉÜ„ÉÉ„ÉóAdd
                                    seen_hashes.add(state_tactic_hash_val)
                                    
                                    # state_tactic_hash „Çπ„ÉÜ„ÉÉ„Éó ËøΩÂä†
                                    step['state_tactic_hash'] = state_tactic_hash_val
                                    current_batch.append(step)
                                    stats['total_steps_after'] += 1
                                    
                                    # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫ ÈÅîdid„Çâ„Éï„Ç°„Ç§„É´ ‰øùÂ≠ò
                                    if len(current_batch) >= batch_size:
                                        save_batch(current_batch, file_counter)
                                        file_counter += 1
                                        current_batch = []
                                        
                                        # „É°„É¢„É™‰ΩøÁî®Èáè „ÉÅ„Çß„ÉÉ„ÇØ
                                        if file_idx % 10 == 0:  # 10„Éï„Ç°„Ç§„É´„Åî„Å® „ÉÅ„Çß„ÉÉ„ÇØ
                                            import psutil
                                            process = psutil.Process()
                                            memory_mb = process.memory_info().rss / 1024 / 1024
                                            stats['memory_usage_mb'] = memory_mb
                                            if verbose:
                                                print(f"   üìä Memory usage: {memory_mb:.1f} MB")
                                
                            except Exception as e:
                                if verbose:
                                    print(f"‚ö†Ô∏è Error processing step: {e}")
                                continue
        
        # ÊÆã„Çä„ÅÆ„Éê„ÉÉ„ÉÅ ‰øùÂ≠òÔºà„Ç∑„Éº„Ç±„É≥„Ç∑„É£„É´„É¢„Éº„Éâ„ÅÆÂ†¥ÂêàonlyÔºâ
        if max_workers == 1 and current_batch:
            save_batch(current_batch, file_counter)
            file_counter += 1
        
        stats['output_files'] = file_counter
        
        # ÂÖ®‰ΩìÁµ±Ë®à Ë®àÁÆó
        stats['processing_time'] = time.time() - start_time
        stats['duplicate_rate'] = (
            stats['duplicate_steps'] / stats['total_steps_before'] * 100
            if stats['total_steps_before'] > 0 else 0.0
        )
        
        # Áµ±Ë®à„É¨„Éù„Éº„Éà Ë°®Á§∫
        print(f"\nüìä Deduplication Summary")
        print(f"   Input files processed: {stats['total_files']}")
        print(f"   Output files created: {stats['output_files']}")
        print(f"   Total steps before: {stats['total_steps_before']:,}")
        print(f"   Total steps after: {stats['total_steps_after']:,}")
        print(f"   Duplicates removed: {stats['duplicate_steps']:,}")
        print(f"   Duplicate rate: {stats['duplicate_rate']:.2f}%")
        print(f"   Processing time: {stats['processing_time']:.2f}s")
        print(f"   Average steps per output file: {stats['total_steps_after'] / stats['output_files']:.0f}")
        if stats['memory_usage_mb'] > 0:
            print(f"   Peak memory usage: {stats['memory_usage_mb']:.1f} MB")
        if stats.get('parallel_processing', False):
            print(f"   Parallel processing: {stats['max_workers']} workers")
            print(f"   Processing speed: {stats['total_steps_before'] / stats['processing_time']:.0f} steps/sec")
        
        # ÊúÄ„ÇÇÈáçË§á Â§ö„ÅÑ„Éè„ÉÉ„Ç∑„É• Ë°®Á§∫
        if stats['duplicate_hash_counts']:
            print(f"\nüîç Top 10 Most Duplicated States:")
            for hash_val, count in stats['duplicate_hash_counts'].most_common(10):
                print(f"   Hash: {hash_val[:16]}... Count: {count}")
        
        # Áµ±Ë®à„É¨„Éù„Éº„Éà ‰øùÂ≠ò
        if report_file:
            try:
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(stats, f, ensure_ascii=False, indent=2)
                print(f"\nüìÑ Statistics saved to: {report_file}")
            except Exception as e:
                print(f"‚ùå Error saving report: {e}")
        
        return stats
        
    finally:
        # ‰∏ÄÊôÇ„Éá„Ç£„É¨„ÇØ„Éà„É™ „ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
        if cleanup_temp and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                if verbose:
                    print(f"üßπ Cleaned up temporary directory: {temp_dir}")
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: Could not clean up temporary directory {temp_dir}: {e}")


def main():
    parser = argparse.ArgumentParser(
        description="Deduplicate GCS data before training",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic GCS deduplication
  python src/training/deduplicate_gcs_data.py \\
    --gcs_bucket fof-data-20251009-milano \\
    --gcs_prefix generated_data/ \\
    --output_dir deduplicated_data
  
  # With detailed report and custom settings
  python src/training/deduplicate_gcs_data.py \\
    --gcs_bucket fof-data-20251009-milano \\
    --gcs_prefix generated_data/ \\
    --output_dir deduplicated_data \\
    --report_file gcs_deduplication_report.json \\
    --batch_size 20000 \\
    --max_workers 4 \\
    --verbose

  # High-performance parallel processing
  python src/training/deduplicate_gcs_data.py \\
    --gcs_bucket fof-data-20251009-milano \\
    --gcs_prefix generated_data/ \\
    --output_dir deduplicated_data \\
    --max_workers 8 \\
    --batch_size 50000 \\
    --verbose
        """
    )
    
    parser.add_argument(
        "--gcs_bucket", 
        type=str, 
        required=True,
        help="GCS bucket name"
    )
    parser.add_argument(
        "--gcs_prefix", 
        type=str, 
        required=True,
        help="GCS prefix for data files"
    )
    parser.add_argument(
        "--output_dir", 
        type=str, 
        default="deduplicated_data",
        help="Output directory for deduplicated files (default: deduplicated_data)"
    )
    parser.add_argument(
        "--temp_dir", 
        type=str, 
        default=None,
        help="Temporary directory for downloading GCS files (default: auto-generated)"
    )
    parser.add_argument(
        "--report_file", 
        type=str, 
        default="gcs_deduplication_report.json",
        help="Path to save deduplication statistics report (default: gcs_deduplication_report.json)"
    )
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="Show detailed processing information"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=10000,
        help="Number of steps per output file (default: 10000)"
    )
    parser.add_argument(
        "--no_memory_efficient", 
        action="store_true",
        help="Disable memory-efficient mode (load all data into memory)"
    )
    parser.add_argument(
        "--max_workers", 
        type=int, 
        default=None,
        help="Maximum number of parallel workers (default: min(cpu_count, 8))"
    )
    parser.add_argument(
        "--no_parallel_download", 
        action="store_true",
        help="Disable parallel download from GCS"
    )
    
    args = parser.parse_args()
    
    # ÈáçË§áÊéíÈô§ ÂÆüË°å
    try:
        stats = deduplicate_gcs_data(
            gcs_bucket=args.gcs_bucket,
            gcs_prefix=args.gcs_prefix,
            output_dir=args.output_dir,
            temp_dir=args.temp_dir,
            report_file=args.report_file,
            verbose=args.verbose,
            batch_size=args.batch_size,
            memory_efficient=not args.no_memory_efficient,
            max_workers=args.max_workers,
            parallel_download=not args.no_parallel_download
        )
        
        if stats:
            print(f"\n‚úÖ GCS deduplication completed successfully!")
            print(f"   Output directory: {args.output_dir}")
            if args.report_file:
                print(f"   Report file: {args.report_file}")
        else:
            print(f"‚ùå GCS deduplication failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"‚ùå Error during GCS deduplication: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
