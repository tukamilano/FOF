#!/usr/bin/env python3
"""
GCSä¸Šã®ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡æ’é™¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
state_hashã‹ã‚‰state_tactic_hashã‚’å†è¨ˆç®—ã—ã¦é‡è¤‡æ’é™¤ã‚’å®Ÿè¡Œ
1Mã‚ªãƒ¼ãƒ€ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã™ã‚‹åŠ¹ç‡çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…
"""
import argparse
import json
import os
import sys
import glob
import time
import hashlib
from typing import List, Dict, Any, Set, Optional, Tuple
from collections import Counter, defaultdict
from tqdm import tqdm
from google.cloud import storage
import tempfile
import shutil
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import threading
from queue import Queue

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

from src.core.state_encoder import format_tactic_string


def state_tactic_hash(premises: List[str], goal: str, tactic: str) -> str:
    """
    çŠ¶æ…‹ã¨tacticã®çµ„ã¿åˆã‚ã›ã®ãƒãƒƒã‚·ãƒ¥
    é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚„ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã«ä½¿ç”¨
    
    Args:
        premises: å‰æã®ãƒªã‚¹ãƒˆ
        goal: ã‚´ãƒ¼ãƒ«
        tactic: æˆ¦ç•¥æ–‡å­—åˆ—
        
    Returns:
        çŠ¶æ…‹ã¨tacticã®çµ„ã¿åˆã‚ã›ã®ãƒãƒƒã‚·ãƒ¥å€¤
    """
    record_str = f"{'|'.join(premises)}|{goal}|{tactic}"
    return hashlib.md5(record_str.encode()).hexdigest()


def download_single_file(args: Tuple) -> Optional[str]:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
    
    Args:
        args: (blob, local_dir) ã®ã‚¿ãƒ—ãƒ«
        
    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
    """
    blob, local_dir = args
    
    try:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        local_filename = os.path.basename(blob.name)
        local_path = os.path.join(local_dir, local_filename)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        blob.download_to_filename(local_path)
        return local_path
        
    except Exception as e:
        print(f"âŒ Error downloading {blob.name}: {e}")
        return None


def download_gcs_data_parallel(gcs_bucket: str, gcs_prefix: str, local_dir: str, 
                              max_workers: int = 4, verbose: bool = False) -> List[str]:
    """
    GCSã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    
    Args:
        gcs_bucket: GCSãƒã‚±ãƒƒãƒˆå
        gcs_prefix: GCSãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        local_dir: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        max_workers: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹
        
    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
    """
    print(f"ğŸ“¥ Downloading data from GCS (parallel)...")
    print(f"   Bucket: {gcs_bucket}")
    print(f"   Prefix: {gcs_prefix}")
    print(f"   Local directory: {local_dir}")
    print(f"   Max workers: {max_workers}")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(local_dir, exist_ok=True)
    
    # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    client = storage.Client()
    bucket = client.bucket(gcs_bucket)
    
    # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã«ãƒãƒƒãƒã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    blobs = bucket.list_blobs(prefix=gcs_prefix)
    json_files = [blob for blob in blobs if blob.name.endswith('.json')]
    
    if not json_files:
        print(f"âŒ No JSON files found in gs://{gcs_bucket}/{gcs_prefix}")
        return []
    
    print(f"ğŸ“ Found {len(json_files)} JSON files in GCS")
    
    # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
    downloaded_files = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’æº–å‚™
        download_tasks = [(blob, local_dir) for blob in json_files]
        
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
        with tqdm(total=len(download_tasks), desc="Downloading files", unit="file") as pbar:
            future_to_blob = {executor.submit(download_single_file, task): task[0] for task in download_tasks}
            
            for future in as_completed(future_to_blob):
                blob = future_to_blob[future]
                try:
                    result = future.result()
                    if result:
                        downloaded_files.append(result)
                        if verbose:
                            print(f"   âœ… Downloaded: {os.path.basename(blob.name)}")
                except Exception as e:
                    print(f"âŒ Error downloading {blob.name}: {e}")
                finally:
                    pbar.update(1)
    
    print(f"âœ… Downloaded {len(downloaded_files)} files")
    return downloaded_files


def download_gcs_data(gcs_bucket: str, gcs_prefix: str, local_dir: str, verbose: bool = False) -> List[str]:
    """
    GCSã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆã€å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    
    Args:
        gcs_bucket: GCSãƒã‚±ãƒƒãƒˆå
        gcs_prefix: GCSãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        local_dir: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹
        
    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
    """
    return download_gcs_data_parallel(gcs_bucket, gcs_prefix, local_dir, max_workers=1, verbose=verbose)


def process_single_file_worker(args: Tuple) -> Dict[str, Any]:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ã€é‡è¤‡æ’é™¤ãªã—ï¼‰
    
    Args:
        args: (file_path,) ã®ã‚¿ãƒ—ãƒ«
        
    Returns:
        å‡¦ç†çµæœã®è¾æ›¸
    """
    file_path, = args
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(file_path, 'r', encoding='utf-8') as f:
            file_data = json.load(f)
        
        processed_steps = []
        file_stats = {
            'file_name': os.path.basename(file_path),
            'total_steps': 0,
            'processing_time': 0.0
        }
        
        start_time = time.time()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‡¦ç†ï¼ˆé‡è¤‡æ’é™¤ãªã—ï¼‰
        for example in file_data:
            for step in example.get('steps', []):
                if step.get('tactic_apply', False):
                    file_stats['total_steps'] += 1
                    
                    # state_tactic_hashã‚’ç”Ÿæˆ
                    premises = step.get('premises', [])
                    goal = step.get('goal', '')
                    tactic_dict = step.get('tactic', {})
                    
                    try:
                        tactic_str = format_tactic_string(tactic_dict)
                        state_tactic_hash_val = state_tactic_hash(premises, goal, tactic_str)
                        
                        # state_tactic_hashã‚’ã‚¹ãƒ†ãƒƒãƒ—ã«è¿½åŠ 
                        step['state_tactic_hash'] = state_tactic_hash_val
                        processed_steps.append(step)
                        
                    except Exception as e:
                        if verbose:
                            print(f"âš ï¸ Error processing step in {file_path}: {e}")
                        continue
        
        file_stats['processing_time'] = time.time() - start_time
        file_stats['processed_steps'] = processed_steps
        
        return file_stats
        
    except Exception as e:
        return {
            'file_name': os.path.basename(file_path),
            'error': str(e),
            'total_steps': 0,
            'processing_time': 0.0,
            'processed_steps': []
        }


def deduplicate_gcs_data(
    gcs_bucket: str,
    gcs_prefix: str,
    output_dir: str,
    temp_dir: Optional[str] = None,
    report_file: str = None,
    verbose: bool = False,
    batch_size: int = 10000,
    memory_efficient: bool = True,
    max_workers: int = None,
    parallel_download: bool = True
) -> Dict[str, Any]:
    """
    GCSä¸Šã®ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ã‚’é™¤å»ã—ã€é‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        gcs_bucket: GCSãƒã‚±ãƒƒãƒˆå
        gcs_prefix: GCSãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        temp_dir: ä¸€æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        report_file: çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        verbose: è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10000ï¼‰
        memory_efficient: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        max_workers: ä¸¦åˆ—å‡¦ç†ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•è¨­å®šï¼‰
        parallel_download: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
    
    Returns:
        é‡è¤‡æ’é™¤çµ±è¨ˆæƒ…å ±
    """
    print(f"ğŸ” Starting GCS data deduplication process...")
    print(f"   GCS bucket: {gcs_bucket}")
    print(f"   GCS prefix: {gcs_prefix}")
    print(f"   Output directory: {output_dir}")
    print(f"   Batch size: {batch_size}")
    print(f"   Memory efficient: {memory_efficient}")
    print(f"   Parallel download: {parallel_download}")
    
    # ä¸¦åˆ—å‡¦ç†ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’è¨­å®š
    if max_workers is None:
        max_workers = min(mp.cpu_count(), 8)  # æœ€å¤§8ãƒ—ãƒ­ã‚»ã‚¹
    print(f"   Max workers: {max_workers}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
    if temp_dir is None:
        temp_dir = tempfile.mkdtemp(prefix="gcs_dedup_")
        cleanup_temp = True
    else:
        os.makedirs(temp_dir, exist_ok=True)
        cleanup_temp = False
    
    try:
        # GCSã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if parallel_download:
            input_files = download_gcs_data_parallel(gcs_bucket, gcs_prefix, temp_dir, 
                                                   max_workers=max_workers, verbose=verbose)
        else:
            input_files = download_gcs_data(gcs_bucket, gcs_prefix, temp_dir, verbose)
        
        if not input_files:
            print(f"âŒ No files downloaded from GCS")
            return {}
        
        print(f"ğŸ“ Processing {len(input_files)} downloaded files")
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡æ’é™¤ç”¨ã®ã‚»ãƒƒãƒˆï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ç”¨ï¼‰
        seen_hashes: Set[str] = set()
        
        # ãƒãƒƒãƒå‡¦ç†ç”¨ã®å¤‰æ•°
        current_batch = []
        file_counter = 0
        
        # çµ±è¨ˆæƒ…å ±
        stats = {
            'total_files': len(input_files),
            'total_steps_before': 0,
            'total_steps_after': 0,
            'duplicate_steps': 0,
            'duplicate_rate': 0.0,
            'output_files': 0,
            'duplicate_hash_counts': Counter(),
            'processing_time': 0.0,
            'memory_usage_mb': 0.0,
            'parallel_processing': True,
            'max_workers': max_workers
        }
        
        start_time = time.time()
        
        def save_batch(batch_data: List[Dict], batch_num: int) -> None:
            """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
            output_file = os.path.join(output_dir, f"deduplicated_batch_{batch_num:05d}.json")
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(batch_data, f, ensure_ascii=False, indent=2)
                if verbose:
                    print(f"   ğŸ’¾ Saved batch {batch_num:05d}: {len(batch_data)} steps")
            except Exception as e:
                print(f"âŒ Error saving batch {batch_num:05d}: {e}")
        
        def process_step(step: Dict) -> Optional[str]:
            """ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰state_tactic_hashã‚’ç”Ÿæˆ"""
            try:
                premises = step.get('premises', [])
                goal = step.get('goal', '')
                tactic_dict = step.get('tactic', {})
                
                # tacticæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
                tactic_str = format_tactic_string(tactic_dict)
                
                # state_tactic_hashã‚’è¨ˆç®—
                return state_tactic_hash(premises, goal, tactic_str)
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ Error processing step: {e}")
                return None
        
        # ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®å®Ÿè£…
        if max_workers > 1:
            print(f"ğŸ”„ Processing files in parallel mode with {max_workers} workers...")
            
            # ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚¿ã‚¹ã‚¯ã‚’æº–å‚™
            file_tasks = [(file_path,) for file_path in input_files]
            
            all_processed_steps = []
            
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ
                with tqdm(total=len(file_tasks), desc="Processing files", unit="file") as pbar:
                    future_to_file = {executor.submit(process_single_file_worker, task): task[0] for task in file_tasks}
                    
                    for future in as_completed(future_to_file):
                        file_path = future_to_file[future]
                        try:
                            result = future.result()
                            
                            # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
                            stats['total_steps_before'] += result['total_steps']
                            
                            # å‡¦ç†ã•ã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—ã‚’åé›†
                            if 'processed_steps' in result:
                                all_processed_steps.extend(result['processed_steps'])
                            
                            if verbose:
                                print(f"   âœ… Processed {result['file_name']}: "
                                      f"{result['total_steps']} steps "
                                      f"({result['processing_time']:.2f}s)")
                            
                        except Exception as e:
                            print(f"âŒ Error processing {file_path}: {e}")
                        finally:
                            pbar.update(1)
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡æ’é™¤ã‚’å®Ÿè¡Œï¼ˆä¸¦åˆ—å‡¦ç†å¾Œï¼‰
            print(f"ğŸ”„ Performing global deduplication on {len(all_processed_steps)} steps...")
            global_unique_steps = []
            global_duplicate_count = 0
            
            for step in tqdm(all_processed_steps, desc="Global deduplication"):
                state_tactic_hash_val = step.get('state_tactic_hash', '')
                
                if state_tactic_hash_val in seen_hashes:
                    global_duplicate_count += 1
                    stats['duplicate_hash_counts'][state_tactic_hash_val] += 1
                else:
                    seen_hashes.add(state_tactic_hash_val)
                    global_unique_steps.append(step)
            
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡è¤‡çµ±è¨ˆã‚’æ›´æ–°
            stats['duplicate_steps'] = global_duplicate_count
            stats['total_steps_after'] = len(global_unique_steps)
            
            print(f"ğŸ’¾ Saving {len(global_unique_steps)} unique steps in batches...")
            
            # ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦ä¿å­˜ï¼ˆé€æ¬¡æ›¸ãè¾¼ã¿ï¼‰
            for i in range(0, len(global_unique_steps), batch_size):
                batch = global_unique_steps[i:i + batch_size]
                save_batch(batch, file_counter)
                file_counter += 1
                
                if verbose and file_counter % 10 == 0:
                    print(f"   ğŸ’¾ Saved batch {file_counter}: {len(batch)} steps")
        
        else:
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆå…ƒã®å®Ÿè£…ï¼‰
            print(f"ğŸ”„ Processing files in sequential mode...")
            
            for file_idx, input_file in enumerate(tqdm(input_files, desc="Processing files")):
                if verbose:
                    print(f"\nğŸ“„ Processing {os.path.basename(input_file)}...")
                
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                try:
                    with open(input_file, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                except Exception as e:
                    print(f"âŒ Error reading {input_file}: {e}")
                    continue
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‡¦ç†
                for example in file_data:
                    for step in example.get('steps', []):
                        if step.get('tactic_apply', False):
                            stats['total_steps_before'] += 1
                            
                            # state_tactic_hashã‚’ç”Ÿæˆ
                            premises = step.get('premises', [])
                            goal = step.get('goal', '')
                            tactic_dict = step.get('tactic', {})
                            
                            try:
                                tactic_str = format_tactic_string(tactic_dict)
                                state_tactic_hash_val = state_tactic_hash(premises, goal, tactic_str)
                                
                                if state_tactic_hash_val in seen_hashes:
                                    # é‡è¤‡ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                    stats['duplicate_steps'] += 1
                                    stats['duplicate_hash_counts'][state_tactic_hash_val] += 1
                                else:
                                    # æ–°ã—ã„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ 
                                    seen_hashes.add(state_tactic_hash_val)
                                    
                                    # state_tactic_hashã‚’ã‚¹ãƒ†ãƒƒãƒ—ã«è¿½åŠ 
                                    step['state_tactic_hash'] = state_tactic_hash_val
                                    current_batch.append(step)
                                    stats['total_steps_after'] += 1
                                    
                                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                                    if len(current_batch) >= batch_size:
                                        save_batch(current_batch, file_counter)
                                        file_counter += 1
                                        current_batch = []
                                        
                                        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
                                        if file_idx % 10 == 0:  # 10ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
                                            import psutil
                                            process = psutil.Process()
                                            memory_mb = process.memory_info().rss / 1024 / 1024
                                            stats['memory_usage_mb'] = memory_mb
                                            if verbose:
                                                print(f"   ğŸ“Š Memory usage: {memory_mb:.1f} MB")
                                
                            except Exception as e:
                                if verbose:
                                    print(f"âš ï¸ Error processing step: {e}")
                                continue
        
        # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’ä¿å­˜ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿ï¼‰
        if max_workers == 1 and current_batch:
            save_batch(current_batch, file_counter)
            file_counter += 1
        
        stats['output_files'] = file_counter
        
        # å…¨ä½“çµ±è¨ˆã‚’è¨ˆç®—
        stats['processing_time'] = time.time() - start_time
        stats['duplicate_rate'] = (
            stats['duplicate_steps'] / stats['total_steps_before'] * 100
            if stats['total_steps_before'] > 0 else 0.0
        )
        
        # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
        print(f"\nğŸ“Š Deduplication Summary")
        print(f"   Input files processed: {stats['total_files']}")
        print(f"   Output files created: {stats['output_files']}")
        print(f"   Total steps before: {stats['total_steps_before']:,}")
        print(f"   Total steps after: {stats['total_steps_after']:,}")
        print(f"   Duplicates removed: {stats['duplicate_steps']:,}")
        print(f"   Duplicate rate: {stats['duplicate_rate']:.2f}%")
        print(f"   Processing time: {stats['processing_time']:.2f}s")
        print(f"   Average steps per output file: {stats['total_steps_after'] / stats['output_files']:.0f}")
        if stats['memory_usage_mb'] > 0:
            print(f"   Peak memory usage: {stats['memory_usage_mb']:.1f} MB")
        if stats.get('parallel_processing', False):
            print(f"   Parallel processing: {stats['max_workers']} workers")
            print(f"   Processing speed: {stats['total_steps_before'] / stats['processing_time']:.0f} steps/sec")
        
        # æœ€ã‚‚é‡è¤‡ãŒå¤šã„ãƒãƒƒã‚·ãƒ¥ã‚’è¡¨ç¤º
        if stats['duplicate_hash_counts']:
            print(f"\nğŸ” Top 10 Most Duplicated States:")
            for hash_val, count in stats['duplicate_hash_counts'].most_common(10):
                print(f"   Hash: {hash_val[:16]}... Count: {count}")
        
        # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
        if report_file:
            try:
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(stats, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ“„ Statistics saved to: {report_file}")
            except Exception as e:
                print(f"âŒ Error saving report: {e}")
        
        return stats
        
    finally:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if cleanup_temp and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                if verbose:
                    print(f"ğŸ§¹ Cleaned up temporary directory: {temp_dir}")
            except Exception as e:
                print(f"âš ï¸ Warning: Could not clean up temporary directory {temp_dir}: {e}")


def main():
    parser = argparse.ArgumentParser(
        description="Deduplicate GCS data before training",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic GCS deduplication
  python src/training/deduplicate_gcs_data.py \\
    --gcs_bucket fof-data-20251009-milano \\
    --gcs_prefix generated_data/ \\
    --output_dir deduplicated_data
  
  # With detailed report and custom settings
  python src/training/deduplicate_gcs_data.py \\
    --gcs_bucket fof-data-20251009-milano \\
    --gcs_prefix generated_data/ \\
    --output_dir deduplicated_data \\
    --report_file gcs_deduplication_report.json \\
    --batch_size 20000 \\
    --max_workers 4 \\
    --verbose

  # High-performance parallel processing
  python src/training/deduplicate_gcs_data.py \\
    --gcs_bucket fof-data-20251009-milano \\
    --gcs_prefix generated_data/ \\
    --output_dir deduplicated_data \\
    --max_workers 8 \\
    --batch_size 50000 \\
    --verbose
        """
    )
    
    parser.add_argument(
        "--gcs_bucket", 
        type=str, 
        required=True,
        help="GCS bucket name"
    )
    parser.add_argument(
        "--gcs_prefix", 
        type=str, 
        required=True,
        help="GCS prefix for data files"
    )
    parser.add_argument(
        "--output_dir", 
        type=str, 
        default="deduplicated_data",
        help="Output directory for deduplicated files (default: deduplicated_data)"
    )
    parser.add_argument(
        "--temp_dir", 
        type=str, 
        default=None,
        help="Temporary directory for downloading GCS files (default: auto-generated)"
    )
    parser.add_argument(
        "--report_file", 
        type=str, 
        default="gcs_deduplication_report.json",
        help="Path to save deduplication statistics report (default: gcs_deduplication_report.json)"
    )
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="Show detailed processing information"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=10000,
        help="Number of steps per output file (default: 10000)"
    )
    parser.add_argument(
        "--no_memory_efficient", 
        action="store_true",
        help="Disable memory-efficient mode (load all data into memory)"
    )
    parser.add_argument(
        "--max_workers", 
        type=int, 
        default=None,
        help="Maximum number of parallel workers (default: min(cpu_count, 8))"
    )
    parser.add_argument(
        "--no_parallel_download", 
        action="store_true",
        help="Disable parallel download from GCS"
    )
    
    args = parser.parse_args()
    
    # é‡è¤‡æ’é™¤ã‚’å®Ÿè¡Œ
    try:
        stats = deduplicate_gcs_data(
            gcs_bucket=args.gcs_bucket,
            gcs_prefix=args.gcs_prefix,
            output_dir=args.output_dir,
            temp_dir=args.temp_dir,
            report_file=args.report_file,
            verbose=args.verbose,
            batch_size=args.batch_size,
            memory_efficient=not args.no_memory_efficient,
            max_workers=args.max_workers,
            parallel_download=not args.no_parallel_download
        )
        
        if stats:
            print(f"\nâœ… GCS deduplication completed successfully!")
            print(f"   Output directory: {args.output_dir}")
            if args.report_file:
                print(f"   Report file: {args.report_file}")
        else:
            print(f"âŒ GCS deduplication failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ Error during GCS deduplication: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
