#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒƒãƒãªã—å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé‡è¤‡æ’é™¤æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ï¼‰
"""
from __future__ import annotations

import argparse
import json
import os
import sys
import glob
import time
from typing import List, Tuple, Dict, Any

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("Warning: wandb not available. Install with: pip install wandb")

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from tqdm import tqdm

from src.core.transformer_classifier import (
    load_tokens_and_labels_from_token_py,
    CharTokenizer,
    TransformerClassifier,
    build_hierarchical_label_mappings,
)
from src.core.state_encoder import parse_tactic_string
from src.core.parameter import (
    get_model_params, get_training_params, 
    get_system_params, get_hierarchical_labels
)
from src.training.inference_hierarchical import evaluate_inference_performance


class SimpleDataset(Dataset):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒãƒƒãƒãªã—ï¼‰"""
    
    def __init__(
        self, 
        data_dir: str,
        tokenizer: CharTokenizer,
        main_to_id: Dict[str, int],
        arg1_to_id: Dict[str, int], 
        arg2_to_id: Dict[str, int],
        max_seq_len: int = 512
    ):
        self.tokenizer = tokenizer
        self.main_to_id = main_to_id
        self.arg1_to_id = arg1_to_id
        self.arg2_to_id = arg2_to_id
        self.max_seq_len = max_seq_len
        
        # é‡è¤‡æ’é™¤æ¸ˆã¿ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        self.data = self._load_batch_data(data_dir)
    
    def _load_batch_data(self, data_dir: str) -> List[Dict[str, Any]]:
        """é‡è¤‡æ’é™¤æ¸ˆã¿ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        data = []
        json_files = glob.glob(os.path.join(data_dir, "deduplicated_batch_*.json"))
        json_files.sort()  # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †åºã‚ˆãèª­ã¿è¾¼ã¿
        
        print(f"Found {len(json_files)} batch files in {data_dir}")
        
        for json_file in json_files:
            print(f"Loading {os.path.basename(json_file)}...")
            with open(json_file, 'r') as f:
                batch_data = json.load(f)
            
            # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥è¿½åŠ ï¼ˆæ—¢ã«é‡è¤‡æ’é™¤æ¸ˆã¿ï¼‰
            data.extend(batch_data)
        
        print(f"Loaded {len(data)} training examples")
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # å…¥åŠ›ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        premises = item['premises']
        goal = item['goal']
        input_ids, attention_mask, segment_ids = self.tokenizer.encode(
            goal, premises, self.max_seq_len
        )
        
        # ã‚¿ã‚¯ãƒ†ã‚£ã‚¯ã‚’è§£æ
        tactic = item['tactic']
        if isinstance(tactic, str):
            tactic_dict = parse_tactic_string(tactic)
        else:
            tactic_dict = tactic
        
        # ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        main_tactic = tactic_dict['main']
        arg1 = tactic_dict['arg1']
        arg2 = tactic_dict['arg2']
        
        # IDã«å¤‰æ›
        main_label = self.main_to_id.get(main_tactic, 0)
        arg1_label = self.arg1_to_id.get(arg1, 0) if arg1 is not None else -1  # -1ã¯ç„¡åŠ¹å€¤
        arg2_label = self.arg2_to_id.get(arg2, 0) if arg2 is not None else -1  # -1ã¯ç„¡åŠ¹å€¤
        
        return input_ids, attention_mask, main_label, arg1_label, arg2_label


def train_single_example(
    model: TransformerClassifier,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    main_label: int,
    arg1_label: int,
    arg2_label: int,
    optimizer: torch.optim.Optimizer,
    criterion: nn.Module,
    arg1_loss_weight: float = 0.8,
    arg2_loss_weight: float = 0.8,
) -> float:
    """å˜ä¸€ã®ä¾‹ã§å­¦ç¿’ã‚’å®Ÿè¡Œ"""
    model.train()
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®å‹¾é…ã‚’ãƒªã‚»ãƒƒãƒˆ
    optimizer.zero_grad()
    
    # ãƒ¢ãƒ‡ãƒ«æ¨è«–
    main_logits, arg1_logits, arg2_logits = model(input_ids, attention_mask)
    
    # æå¤±è¨ˆç®—ï¼ˆç„¡åŠ¹å€¤-1ã‚’é™¤å¤–ï¼‰
    main_loss = criterion(main_logits, torch.tensor([main_label], device=main_logits.device))
    
    # arg1ã®æå¤±è¨ˆç®—ï¼ˆç„¡åŠ¹å€¤-1ã‚’é™¤å¤–ï¼‰
    arg1_loss = 0.0
    if arg1_label != -1:
        arg1_loss = criterion(arg1_logits, torch.tensor([arg1_label], device=arg1_logits.device))
    
    # arg2ã®æå¤±è¨ˆç®—ï¼ˆç„¡åŠ¹å€¤-1ã‚’é™¤å¤–ï¼‰
    arg2_loss = 0.0
    if arg2_label != -1:
        arg2_loss = criterion(arg2_logits, torch.tensor([arg2_label], device=arg2_logits.device))
    
    # ç·æå¤±ï¼ˆé‡ã¿ä»˜ãï¼‰
    total_loss = main_loss + arg1_loss_weight * arg1_loss + arg2_loss_weight * arg2_loss
    
    # é€†ä¼æ’­
    total_loss.backward()
    optimizer.step()
    
    return total_loss.item()


def main():
    parser = argparse.ArgumentParser(description="Simple training script for hierarchical tactic classifier")
    parser.add_argument("--data_dir", type=str, default="deduplicated_data", help="deduplicated data directory")
    parser.add_argument("--learning_rate", type=float, default=3e-4, help="learning rate")
    parser.add_argument("--num_epochs", type=int, default=1, help="number of epochs")
    parser.add_argument("--device", type=str, default="auto", help="device")
    parser.add_argument("--save_path", type=str, default="models/simple_model.pth", help="model save path")
    parser.add_argument("--max_seq_len", type=int, default=256, help="maximum sequence length")
    parser.add_argument("--use_wandb", action="store_true", help="use wandb for logging")
    parser.add_argument("--wandb_project", type=str, default="fof-simple-training", help="wandb project name")
    parser.add_argument("--wandb_run_name", type=str, default=None, help="wandb run name")
    parser.add_argument("--arg1_loss_weight", type=float, default=0.8, help="weight for arg1 loss")
    parser.add_argument("--arg2_loss_weight", type=float, default=0.8, help="weight for arg2 loss")
    parser.add_argument("--random_seed", type=int, default=42, help="random seed for reproducibility")
    parser.add_argument("--log_frequency", type=int, default=1000, help="log training loss every n examples")
    parser.add_argument("--save_frequency", type=int, default=10000, help="save model every n examples")
    
    # æ¨è«–è©•ä¾¡é–¢é€£ã®å¼•æ•°
    parser.add_argument("--inference_eval_examples", type=int, default=100, help="number of examples for inference evaluation")
    parser.add_argument("--inference_max_steps", type=int, default=30, help="max steps for inference evaluation")
    parser.add_argument("--inference_temperature", type=float, default=1.0, help="temperature for inference evaluation")
    
    args = parser.parse_args()
    
    # å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
    print("ğŸš€ Command line arguments:")
    print(f"   Script: {sys.argv[0]}")
    print(f"   Arguments: {' '.join(sys.argv[1:])}")
    print("=" * 60)
    
    # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š
    import random
    import numpy as np
    random.seed(args.random_seed)
    np.random.seed(args.random_seed)
    torch.manual_seed(args.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.random_seed)
        torch.cuda.manual_seed_all(args.random_seed)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    model_params = get_model_params()
    training_params = get_training_params()
    system_params = get_system_params()
    hierarchical_labels = get_hierarchical_labels()
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    
    print(f"Using device: {device}")
    
    # å®Ÿè¡Œè¨­å®šã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
    print("\nğŸ“‹ Training Configuration:")
    print(f"   Data directory: {args.data_dir}")
    print(f"   Learning rate: {args.learning_rate}")
    print(f"   Number of epochs: {args.num_epochs}")
    print(f"   Max sequence length: {args.max_seq_len}")
    print(f"   Random seed: {args.random_seed}")
    print(f"   Use wandb: {args.use_wandb}")
    if args.use_wandb:
        print(f"   Wandb project: {args.wandb_project}")
        print(f"   Wandb run name: {args.wandb_run_name}")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
    data_dir = os.path.join(project_root, args.data_dir)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(data_dir):
        print(f"âŒ Data directory not found: {data_dir}")
        print("Please run: python src/training/deduplicate_generated_data.py")
        return
    
    # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    batch_files = glob.glob(os.path.join(data_dir, "deduplicated_batch_*.json"))
    if not batch_files:
        print(f"âŒ No deduplicated batch files found in {data_dir}")
        print("Please run: python src/training/deduplicate_generated_data.py")
        return
    
    print(f"âœ… Using deduplicated data from: {data_dir}")
    print(f"   Found {len(batch_files)} batch files")
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿
    token_py_path = os.path.join(project_root, "src", "core", "fof_tokens.py")
    base_tokens, _ = load_tokens_and_labels_from_token_py(token_py_path)
    
    # éšå±¤åˆ†é¡ç”¨ã®ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
    main_to_id, arg1_to_id, arg2_to_id, id_to_main, id_to_arg1, id_to_arg2 = build_hierarchical_label_mappings(
        hierarchical_labels.main_tactics,
        hierarchical_labels.arg1_values,
        hierarchical_labels.arg2_values
    )
    
    print(f"Main tactics: {len(id_to_main)} classes")
    print(f"Arg1 values: {len(id_to_arg1)} classes")
    print(f"Arg2 values: {len(id_to_arg2)} classes")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    tokenizer = CharTokenizer(
        base_tokens=base_tokens,
        add_tactic_tokens=model_params.add_tactic_tokens,
        num_tactic_tokens=model_params.num_tactic_tokens
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    print("ğŸ“Š Creating SimpleDataset")
    dataset = SimpleDataset(
        data_dir=data_dir,
        tokenizer=tokenizer,
        main_to_id=main_to_id,
        arg1_to_id=arg1_to_id,
        arg2_to_id=arg2_to_id,
        max_seq_len=args.max_seq_len
    )
    
    if len(dataset) == 0:
        print("No training data found. Please check the deduplicated data directory.")
        return
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = TransformerClassifier(
        vocab_size=tokenizer.vocab_size,
        pad_id=tokenizer.pad_id,
        max_seq_len=args.max_seq_len,
        d_model=model_params.d_model,
        nhead=model_params.nhead,
        num_layers=model_params.num_layers,
        dim_feedforward=model_params.dim_feedforward,
        dropout=model_params.dropout,
        num_main_classes=len(id_to_main),
        num_arg1_classes=len(id_to_arg1),
        num_arg2_classes=len(id_to_arg2),
    )
    
    print(f"Model vocab_size: {tokenizer.vocab_size}")
    print(f"Model pad_id: {tokenizer.pad_id}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
    model = model.to(device)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨æå¤±é–¢æ•°ã‚’ä½œæˆ
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)
    criterion = nn.CrossEntropyLoss()
    
    # wandbåˆæœŸåŒ–
    if args.use_wandb and WANDB_AVAILABLE:
        run_name = args.wandb_run_name or f"simple_training_{int(time.time())}"
        wandb.init(
            project=args.wandb_project,
            name=run_name,
            config={
                "data_dir": args.data_dir,
                "learning_rate": args.learning_rate,
                "num_epochs": args.num_epochs,
                "max_seq_len": args.max_seq_len,
                "device": str(device),
            }
        )
        print(f"Wandb initialized: {args.wandb_project}/{run_name}")
    elif args.use_wandb and not WANDB_AVAILABLE:
        print("Warning: wandb requested but not available. Continuing without logging.")
    
    # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆæ¨è«–è©•ä¾¡ç”¨ï¼‰
    label_mappings = {
        'main_to_id': main_to_id,
        'arg1_to_id': arg1_to_id,
        'arg2_to_id': arg2_to_id,
        'id_to_main': id_to_main,
        'id_to_arg1': id_to_arg1,
        'id_to_arg2': id_to_arg2
    }
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print(f"\nğŸš€ Starting simple training for {args.num_epochs} epochs...")
    print(f"ğŸ“Š Training data: {len(dataset)} examples")
    print(f"ğŸ“Š Learning rate: {args.learning_rate}")
    print(f"ğŸ“Š Log frequency: every {args.log_frequency} examples")
    print(f"ğŸ“Š Save frequency: every {args.save_frequency} examples")
    print("=" * 60)
    
    # å­¦ç¿’é–‹å§‹å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–è©•ä¾¡
    print(f"\nğŸ” Evaluating baseline inference performance (before training)...")
    baseline_success_rate, baseline_avg_steps = evaluate_inference_performance(
        model, tokenizer, label_mappings, device, args.max_seq_len,
        num_examples=args.inference_eval_examples, 
        max_steps=args.inference_max_steps, 
        temperature=args.inference_temperature,
        difficulty=0.7,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®difficultyå€¤ã‚’ä½¿ç”¨
        max_depth=4,  # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ™‚ã¨åŒã˜max_depthå€¤ã‚’ä½¿ç”¨
        seed=42  # å†ç¾æ€§ã®ãŸã‚å›ºå®šã‚·ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    )
    print(f"  Baseline inference success rate: {baseline_success_rate:.3f}")
    print(f"  Baseline inference avg steps (when solved): {baseline_avg_steps:.2f}")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã‚’wandbã«è¨˜éŒ²
    if args.use_wandb and WANDB_AVAILABLE:
        wandb.log({
            "inference/success_rate": baseline_success_rate,
            "inference/avg_steps": baseline_avg_steps
        })
    
    print("=" * 60)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    total_examples = 0
    epoch_losses = []
    
    # ç›´è¿‘log_frequencyåˆ†ã®æå¤±ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã®ã‚­ãƒ¥ãƒ¼
    recent_losses = []
    
    for epoch in range(args.num_epochs):
        print(f"\nğŸš€ Starting epoch {epoch+1}/{args.num_epochs}")
        
        # ã‚¨ãƒãƒƒã‚¯å†…ã®æå¤±ã‚’è¨˜éŒ²
        epoch_loss = 0.0
        num_examples = 0
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        indices = list(range(len(dataset)))
        random.shuffle(indices)
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ä½œæˆ
        pbar = tqdm(indices, desc=f"Epoch {epoch+1}", unit="example")
        
        for example_idx, data_idx in enumerate(pbar):
            # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            input_ids, attention_mask, main_label, arg1_label, arg2_label = dataset[data_idx]
            
            # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            input_ids = input_ids.unsqueeze(0).to(device)  # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
            attention_mask = attention_mask.unsqueeze(0).to(device)  # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
            
            # å˜ä¸€ã®ä¾‹ã§å­¦ç¿’
            loss = train_single_example(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                main_label=main_label,
                arg1_label=arg1_label,
                arg2_label=arg2_label,
                optimizer=optimizer,
                criterion=criterion,
                arg1_loss_weight=args.arg1_loss_weight,
                arg2_loss_weight=args.arg2_loss_weight,
            )
            
            epoch_loss += loss
            num_examples += 1
            total_examples += 1
            
            # ç›´è¿‘log_frequencyåˆ†ã®æå¤±ã‚’è¨˜éŒ²
            recent_losses.append(loss)
            if len(recent_losses) > args.log_frequency:
                recent_losses.pop(0)  # å¤ã„æå¤±ã‚’å‰Šé™¤
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
            pbar.set_postfix({'Loss': f'{loss:.4f}', 'Avg Loss': f'{epoch_loss / num_examples:.4f}'})
            
            # æŒ‡å®šã•ã‚ŒãŸé »åº¦ã§wandbã«ãƒ­ã‚°
            if args.use_wandb and WANDB_AVAILABLE and total_examples % args.log_frequency == 0:
                recent_avg_loss = sum(recent_losses) / len(recent_losses) if recent_losses else 0.0
                wandb.log({
                    "loss": recent_avg_loss,  # ç›´è¿‘log_frequencyåˆ†ã®å¹³å‡
                    "avg_loss": epoch_loss / num_examples  # 1ã‚¨ãƒãƒƒã‚¯å…¨ä½“ã®å¹³å‡
                })
            
            # æŒ‡å®šã•ã‚ŒãŸé »åº¦ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            if total_examples % args.save_frequency == 0:
                checkpoint_path = f"models/simple_model_checkpoint_{total_examples}.pth"
                os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
                torch.save(model.state_dict(), checkpoint_path)
                print(f"\nğŸ’¾ Checkpoint saved: {checkpoint_path}")
        
        # ã‚¨ãƒãƒƒã‚¯ã®å¹³å‡æå¤±ã‚’è¨ˆç®—
        avg_epoch_loss = epoch_loss / num_examples if num_examples > 0 else 0.0
        epoch_losses.append(avg_epoch_loss)
        
        print(f"Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}")
        
        # æ¨è«–æ€§èƒ½ã‚’è©•ä¾¡ï¼ˆæ¯ã‚¨ãƒãƒƒã‚¯ï¼‰
        print(f"\nğŸ” Evaluating inference performance after epoch {epoch+1}...")
        inference_success_rate, inference_avg_steps = evaluate_inference_performance(
            model, tokenizer, label_mappings, device, args.max_seq_len,
            num_examples=args.inference_eval_examples, 
            max_steps=args.inference_max_steps, 
            temperature=args.inference_temperature,
            difficulty=0.7,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®difficultyå€¤ã‚’ä½¿ç”¨
            max_depth=4  # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ™‚ã¨åŒã˜max_depthå€¤ã‚’ä½¿ç”¨
        )
        print(f"  Inference success rate: {inference_success_rate:.3f}")
        print(f"  Inference avg steps (when solved): {inference_avg_steps:.2f}")
        
        # wandbã«ãƒ­ã‚°
        if args.use_wandb and WANDB_AVAILABLE:
            wandb.log({
                "epoch": epoch + 1,
                "epoch_loss": avg_epoch_loss,
                "inference/success_rate": inference_success_rate,
                "inference/avg_steps": inference_avg_steps
            })
    
    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)
    torch.save(model.state_dict(), args.save_path)
    
    print(f"\nğŸ‰ Training completed!")
    print(f"ğŸ“ Model saved to: {args.save_path}")
    print(f"ğŸ“Š Total examples processed: {total_examples}")
    print(f"ğŸ“Š Average loss per epoch: {[f'{loss:.4f}' for loss in epoch_losses]}")
    
    # wandbçµ‚äº†
    if args.use_wandb and WANDB_AVAILABLE:
        wandb.finish()
        print("ğŸ“ˆ Wandb logging completed!")


if __name__ == "__main__":
    main()
